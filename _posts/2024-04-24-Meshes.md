---
title: 3D models
---

In this chapter we kickstart the section of this course regarding the *appearance* of modern video games by talking about **3D models**, the bread and butter of every 3D real-time graphics application.
They represent 3D **surfaces described by polygons** and enriched with *textures* and *materials* (*see following chapters*) and are the 3D counterpart of 2D sprites.
We should specify by now that the models used in video games are what's called **low-poly**, i.e. relatively low resolution with respects to the ones used in movies or other offline rendering situations because of the strict performance constraints.

{% include responsive-image.html path="assets/images/09_LowPoly.png" alt="Examples of low poly models" max-height="170px"%}

In abstract the surfaces that define 3D models can take any shape or form, but how are they actually represented in video games?
Let's find out in the following paragraphs.

## Meshes

The most common and accepted way of representing the visual appearance of 3D objects are **triangle meshes** (*or simplicial meshes*), sets of adjacent triangles that approximate surfaces: more specifically, a triangle mesh is constituted of a set of **vertices** connected by **edges** to form **faces**.
Mathematically we can see each vertex as a "sample" of the original surface connected to nearby samples to recreate an approximation of the 3D object: this is why the *resolution* of a triangle mesh is dictated by the number of its faces.
Note that we didn't require the vertices to be equally spread across the surface and in fact the resolution of a triangle mesh can be *adaptive*, allotting more faces where greater detail is needed.

{% include responsive-image.html path="assets/images/09_Mesh.png" alt="Examples of triangle mesh" max-height="130px" %}

Each triangle mesh (*simply "mesh" from now on*) consists of three components:

- **Geometry**: the set of $$(x, y, z)$$ positions of the vertices that represents the sampling of the surface.
    These coordinates for each vertex are given, by definition, in *Local space*.

{% include responsive-image.html path="assets/images/09_Geometry.png" alt="A mesh's geometry represented as a cloud of points" max-height="100px" %}

- **Connectivity** (*or **topology***): the set of faces connecting the vertices, just as in a graph nodes are connected by edges.
    In the case of a triangle mesh these faces are *triangles*, a shape that is chosen for video games because of how GPU friendly it is, which allows it to speed up rendering, but faces could also be general polygons (*eg. quad meshes use quadrilaterals*).

{% include responsive-image.html path="assets/images/09_Connectivity.png" alt="A mesh's connectivity represented as a graph" max-height="100px" %}

- **Attributes**: any arbitrary quantity that varies across the surface, like color, material, normal etc.
    This data is sampled at the vertices and will then be interpolated across the faces to approximate the original surface (*see later*).

{% include responsive-image.html path="assets/images/09_Attributes.png" alt="A mesh's attributes represented as a data on the graph nodes" max-height="120px" %}

To represent the mesh in our game, all of this data needs to be recorded in an appropriate data structure.
The most common choice for meshes is the **indexed mesh**, which is a pair of tables:

- an **array of vertices**, each stored by recording *both its position (geometry) and its attributes*, which are the same for every vertex (the same fields, not the same values);
- an **array of triangles**, each stored as a *triplet of indices* referring to the vertices array: each triangle keeps track of which rows in the other table describe its vertices.

{% include responsive-image.html path="assets/images/09_IndexedMesh.png" alt="A mesh's attributes represented as a data on the graph nodes" max-height="120px" %}

These two arrays can be seen as *buffers* to send to the GPU's VRAM each time we want to draw an object on screen, an event that is referred to as a **draw call**.
Since **rendering time is linear with the mesh's resolution** we can see why video games use "low-poly" models: reducing the number of faces (and equivalently of the number of vertices, since typically $$n°vertices = 2 \cdot n°faces$$) increases rendering performance.
Note that the fact that *resolution can be adaptive*, i.e. we can have denser vertices and smaller faces in some areas of the model and sparser vertices and larger faces in others, makes it so that we can make good use of the limited *triangle budget* to still obtain good looking results.
Moreover, the limit for "low-poly" models has drastically increased over the years: things that were considered high-poly 10 years ago (eg. $$10^5$$ triangles) are now on par with low-poly models.

### Mesh attributes

Let's talk a bit more about **mesh attributes**.
We've said that these are attributes of the original surface that have been sampled at each vertex position and stored within them: each vertex shares the same collection of attributes which can be either vectors, versors or scalars.
During rendering these attributes are then **interpolated** over each face of the mesh using **barycentric coordinates** to try and recreate the original appearance of the surface.

Let's see an example of this interpolation using vertex color as an attribute to interpolate.
Given triangle $$T$$ and its vertices $$p_0, p_1, p_2$$, for each point $$q \in T$$ there exists a *unique* set of weights $$k_0, k_1, k_2$$ such that:

$$
\begin{cases}
q = k_0 p_0 + k_1 p_1 + k_2 p_2 \\
k_0 + k_1 + k_2 = 1
\end{cases}
$$

i.e. the point $$q$$ can be obtained as a *linear interpolation of the vertices*.
The values $$(k_0, k_1, k_2)$$ are the so-called *barycentric coordinates* of $$q$$ in $$T$$, which are then **used as weights for interpolating attributes**!
In our color example, if we call $$RGB_0, RGB_1, RGB_2$$ the vectors representing the color in the three vertices of the triangle, then point $$q$$ is automatically assigned a mix of these colors:

$$RGB_q = k_0 RGB_0 + k_1 RGB_1 + k_2 RGB_2$$

{% include responsive-image.html path="assets/images/09_Interpolation.png" alt="Example of attribute interpolation across a triangle" max-height="120px" %}

This way each point on a triangle will have values for its attributes which are *most similar to the vertex the point is closest to* but also influenced by the other vertices.
This method gives attributes **C0 continuity** across faces, but *C1 discontinuity*: this means the transition of attributes will be continuous across faces but the speed of change may vary quite a bit between adjacent faces.

Now that we've seen how this interpolation works we can introduce some **common attribute types** in 3D video games, categories of attributes that are recurring and almost always used to improve a mesh's look or functionality.
Some examples are:

- **color**, an RGB value indicating the color in the vertex: this is however not so used anymore as it only allows faces to have flat color gradients, being often replaced by *color maps* (*see textures chapter*);

- **texture coordinates**, a set of 2D coordinates that identify the vertex position in any textures associated with the object (*see textures chapter*);

- **normal**, a versor expressed in *Local space* that will be used during lighting calculations to better shade the model and create the illusion of a smooth surface (*see later*);

- **tangent direction**, again used for rendering and lighting (*see textures chapter*);

- **bone links**, a set of weights used to perform *skeletal animations* (*see animations chapter*).

#### Per-vertex normals

Let's now explore the concept of **per-vertex normals** a little bit more in depth.
Initially it doesn't make any sense: up until now we've defined normals only for surfaces, how is it possible that single points have a normal associated with them?
The idea is that these vertex normals represent the original **surface orientation** and will be used in *lighting computations* to create the illusion of **smoothness**.
These normal values can be computed automatically from the geometry (*see later*) but they're an *asset* in every right: an artist will be in control of how shading is done on the mesh thanks to normals.

{% include responsive-image.html path="assets/images/09_Normals.png" alt="Example of the normal vectors in each vertex of a mesh" max-height="120px" %}

The problem is that, geometrically, mesh faces are flat: the normal is constant across the face and discontinuous across adjacent triangles, which makes the edges between them very *"sharp"* and creates an effect called **flat shading**.
Usually, though, this isn't the surface we intended to represent: the flatness is only a byproduct of the mesh discretization we performed when sampling the original surface to create the model.
Fortunately we can repair this visual artifact by using **normals recorded at each vertex**: when interpolated across the faces as any other attribute this will create *continuously varying normals* across the surface, which the lighting computations will use to create the illusion of a smooth, curved surface, which we call **smooth shading**.
This smoothness is just a trick of light though: as we can see from the figure, even when using smooth shading the silhouette of the model has sharp edges.

{% include responsive-image.html path="assets/images/09_VertexNormals.png" alt="Contrast between flat and smooth shading" max-height="200px" %}

This is not to say that **hard edges** or creases aren't possible with per-vertex normals, but since attribute interpolation is hardwired in the GPU and can't be turned off for specific vertices they must instead be re-created using a special techniques.
What we want is basically an *edge where the normal isn't continuous* or, more generally, a **discontinuity of attributes over an edge**.

{% include responsive-image.html path="assets/images/09_HardEdges.png" alt="Difference between hard edges and soft edges visualized" max-height="200px" %}

This kind of discontinuity can be achieved with **vertex seams**, *two coinciding vertices with different values for the attributes*, in this case the normals.
By having two vertices share their position but not their attributes we can have jumps in the attribute interpolation since two adjacent triangles will use a different copy of the vertex: this of course will result in *a bit of data replication*, a price that can well be paid since most attributes won't need discontinuities and for those that do we'll only have a few seams.

{% include responsive-image.html path="assets/images/09_VertexSeams.png" alt="Example of a vertex seam" max-height="150px" %}

### Life of a mesh

We've said before that to render a mesh we have to first transfer its data on the GPU VRAM and then issue a draw call, usually with the use of an API, but how is the mesh transported from a file on disk up to the GPU?
The **life cycle of a mesh** looks something like this:

1. The mesh is **imported from a file** on disk (*or from a remote server*) to the CPU RAM.
2. While on RAM the mesh may undergo some simple **pre-processing steps** to prepare it for rendering: these may include normals computations, tangent computations or the baking of lighting.
3. The mesh is then **loaded on GPU** and rendered each frame.

{% include responsive-image.html path="assets/images/09_MeshLife.png" alt="Life cycle of a mesh" max-height="150px" %}

Each of these phases is connected to a memory structure in which the mesh data can reside: when on disk the mesh is recorded in a **mesh file**, using one of the many available mesh formats that try to balance the storage cost with the number of attributes they can contain and the time they take to import on RAM.
There isn't a single file format for 3D meshes, but rather a plethora of options that each have their pro's and cons: some examples are *.obj* (indexed list and little more), *.fbx*, *.dae* and *.glTF*.
The disk doesn't have much memory limitations and it can therefore store a *large number of mesh files*.

Once loaded in RAM the mesh takes the form of a **mesh object**, an internal representation of a mesh that can be constructed in a number of different ways: game engines need to decide which attributes to store, which storage formats to use (*floats, bytes, doubles...*) and which **pre-processing steps** to offer, if any, which if present are usually performed at load time.
A structure resembling an indexed mesh is a common choice for this internal representation.
Notice that RAM memory is much more scarce than disk memory, so a *fewer mesh objects* can be kept in memory at any single time.

When transferred to GPU VRAM the mesh is stored in *buffers* which are either called **Vertex Buffer Objects** (VBO) or **Vertex Arrays** depending on the rendering API used: they're basically tables containing the indexed mesh data.
These objects have a very specific format based on the choices the game engine operated, including choosing the size of each attribute and index: we could for example use 8 bits per color channel, 16 bit floats for each coordinate of the position, 16 bit integers for vertex indices and so on.
When choosing these parameters we have to of course balance accuracy of the mesh description with storage cost: GPU RAM is the *most scarce* resource in this pipeline, meaning we can only have *very few* VBOs in it at any time.
Transferring data from the central RAM to the GPU's VRAM is also a very taxing endeavour, so every bit of optimization we can bolster up counts.

{% include responsive-image.html path="assets/images/09_MeshData.png" alt="Representation of the ratio of different mesh data stages" max-height="150px" %}

### Mesh processing

We've already mentioned **mesh processing** as an optional step to take when loading a mesh from disk: let's talk a little bit more about it.
The basic idea is to prepare the mesh for rendering and optimize its representation: mesh processing entails a whole range of algorithms that generate or process meshes in various ways, i.e. *algorithms that have meshes as inputs and/or outputs*.
Some examples include:

- *Normal generation*: create per-vertex normals for lighting (*see later*);
- *Polygonal reduction / Simplification*: turn a hi-res mesh to a low-poly one for rendering;
- *Light baking*: pre-compute lighting for effects like ambient occlusion or static objects;
- *UV map construction*: construct the structure necessary for texturing (*see texture chapter*);
- *Texturing*: create different types of textures for the mesh;
- *Rigging / Skinning / Animation*: create the data structures used to animate the model;
- *so many more...*

#### Normal generation

From this plethora of different algorithms let's look more closely at one that is particularly interesting for meshes: **normals generation**.
Although it isn't that common anymore, sometimes we could load a mesh from disk and find that its vertices don't have per-vertex normals: the solution is then to *compute the normals of each vertex as the **mix of faces' normals***.
This process needs two separate steps:

1. **Compute the faces' normals**: to do so we can simply compute the *normalized cross product* between two vectors that represent two triangle edges.
    Notice how here the *order in which vertices are stored* in the array of triangles is extremely important since it will determine whether the normal faces inward or outward from the surface: vertices must be **ordered consistently** throughout the mesh, for example following counterclockwise order when looking at the surface from the outside.
    A good way to check if the order is consistent is that when two triangles share an edge it should be traversed in different directions in the two faces (*eg. one from A to B and the other from B to A*).

$$\hat{n} = \frac{\vec{e}_1 \times \vec{e}_2}{\|\vec{e}_1 \times \vec{e}_2\|}$$

{% include responsive-image.html path="assets/images/09_NormalConstruction.png" alt="Representation of how face normals are computed" max-height="100px" %}

{:start="2"}
2. **Cumulate the normals in each vertex**: we can now construct the per-vertex normals by averaging the normals of all faces to which each vertex belongs.

$$\hat{n}_v = \frac{\hat{n}_0 + \dots + \hat{n}_k}{\|\hat{n}_0 + \dots + \hat{n}_k\|}$$

{% include responsive-image.html path="assets/images/09_NormalAverage.png" alt="Representation of how face normals are interpolated into per-vertex normals" max-height="100px" %}

#### Mesh simplification

Another interesting thing we can do when processing meshes is **simplifying** them, a process that is also known as *polygonal reduction*, *decimation* or *mesh coarsening*.
The idea is to take a high-resolution mesh and automatically turn it into a low-poly one by iteratively eliminating vertices and faces while usually keeping track of two parameters: the **target face count** we want to get and the **maximum error** we can accept, defined as the distance of the new mesh from the original one.

{% include responsive-image.html path="assets/images/09_Simplification.png" alt="Example of mesh simplification" max-height="150px" %}

Many different approaches to this problem are studied in the field of Geometry Processing: some can be *adaptive*, using more triangles where needed, and some are not; some can keep the mesh *topology* while others don't; some can be *streamable* and some cannot.
We should note that these algorithms work best when taking a really high-resolution mesh and turning it into a low-poly one, while they begin to struggle as the polygon count of the starting model gets lower and lower: using a 2D comparison, these algorithms are like downsampling methods that reduce the size of 2D images but won't result in pixel art!
When the starting mesh is already low-poly the safest bet is to hire an artist for the job instead.

### LOD pyramids

Going back a bit to mesh definition, we've already seen how a *mesh's resolution affects performance*: the more detailed the mesh the longer it will take for the GPU to render.
The problem is that in interactive applications like video games we have to **guarantee a certain level of detail** for almost all meshes since the user may move the camera close to them and would notice if the models were too crude.
How do we do this without using high-res meshes for each object, which would surely ruin performance?

The idea is to **use less polygons for objects that are far away** from the camera, switching to more high-resolution models as the point of view get closer and closer: this technique is called **Level of Detail** (**LOD**) and basically associates a "**pyramid**" of progressively less detailed meshes to each model, which we call *LOD levels*.
Each of these levels will be used to represent the model depending on how far away it is from the camera, hopefully achieving a *uniform polygon count per pixel on screen*: close object will occupy a great deal of the screen space and will therefore need more polygons, while far away objects are so tiny in term of pixels covered that they can be represented with just a few polygons.

{% include responsive-image.html path="assets/images/09_LoD.png" alt="Example of the different levels of detail of a horse model" max-height="200px" %}

This of course **helps performance** but isn't it a *waste of memory*?
After all we're now storing several meshes instead of just one, each of which will need its indexed mesh structure with two tables.
Although this is true, in practice it doesn't prove to be a problem: if for example at each level we reduce the polygon count to $$1/4$$ of the previous level, which is a reasonable reduction, and even admitting that a model had infinite LOD levels this would still result in an *increase of $$1/3$$ in total memory*.
As a matter of fact, if we call $$N$$ the number of polygons at level $$0$$ (*most detailed one*) we have:

$$N_{tot} = N + \frac{1}{4}N + \frac{1}{16}N + \dots = N  \cdot\sum_{i=0}^{\infty}{\left(\frac{1}{4}\right)^i} = N \cdot \frac{4}{3} = N + \frac{1}{3}N$$

Now that we've proved how good of a technique this is we need to clarify two aspects of it:

- **How to produce LOD levels** (*aka "LOD-ing"*): basically, how to construct all the different meshes that represent the model at each level of detail during *asset creation*.
    Usually the process starts from LOD-0 and iteratively reduces the polygon count to produce the other levels either *automatically with mesh simplification algorithms* or by hand, which is often required for very coarse LODs.

- **How to swap LOD levels**: i.e. how to choose which LOD level to use for the model *dynamically at runtime*.
    This can be done depending on **observer distance** and/or on **rendering workload**, reducing LODs when rendering is lagging to guarantee a certain number of FPS.
    If we do it purely based on distance from the camera the most basic approach is to use a *fixed LOD for each interval of distance*: this isn't ideal however as it creates popping artefacts when the camera rides the line separating two intervals, causing a distracting (and expensive) back-and-forth between two LOD levels.
    A common solution is to instead use **different thresholds to increase and decrease LOD**, basically creating a "twilight zone" where both LODs could be used so that even when riding the line between intervals we won't keep switching LODs as the threshold for the opposite switch is different.

{% include responsive-image.html path="assets/images/09_LODSwap.png" alt="Example of different thresholds for increasing and decreasing LODs" max-height="100px" %}

### Hi-res mesh formats: Nanites

In recent years the indexed mesh structure for representing models has begun to feel a bit *obsolete*, opening the state for **alternative high-resolution mesh formats** to be proposed.
While still trying to approximate complex surfaces, most of them feature some or all of the following innovations:

- *Cheaper per-triangle VRAM cost*: this is usually done by compressing the mesh and decompressing it on-the-fly during rendering using appropriate algorithms;

- *Cheaper per-triangle rendering cost*: usually done by also taking real-time raytracing into account;

- ***Multiresolution***: this basically *intrinsic LODs* that can be swapped on-the-fly during rendering, with some newer formats that also allow *different resolution levels across the mesh*;

- *Reduced need for UV maps* (*see next chapter*).

Two of these alternative formats are quite interesting: **Nanites** by Epic Games and **Micro-meshes** by Nvidia.
We postpone for now the discussion on the latter since it requires the knowledge of textures and we explore Nanites instead.

The basic concept of Nanites is to not have LODs for the whole model but instead **local LODs** that allow us to increase the resolution of the part of the mesh that is closer to the camera.
This is done by representing the model through a **tree of patches**, *small and very optimized sub-meshes of 128 triangles*, where **each node is a lower resolution approximation of the union of its children**: this way the leaves collectively represent the LOD-0 of the whole mesh while the root is a very crude approximation of the whole model.
All patches having the same resolution allows for optimizations in the GPU and with this structure we can now draw **mixed resolution** versions of the model by selecting patches at different levels of the tree and rendering them separately.

{% include responsive-image.html path="assets/images/09_NanitesPatches.png" alt="Example of a Nanite tree" max-height="150px" %}

In reality things are a bit more complex: Nanites are not trees of patches but rather *direct acyclic graphs* of patches where nodes that share the same children collectively approximate the same surface as said children with the *same exact boundary*.
This allows us to render the model at mixed resolution while *avoiding cracks in the surface* that could be created by approximations.

{% include responsive-image.html path="assets/images/09_NanitesBoundaries.png" alt="Visualization of different levels of the nanite DAG having the same boundaries" max-height="150px" %}

## Creating 3D models

Now that we've seen how to manage meshes we can ask ourselves: how are they created?
3D models are **assets** just like any other and can be created mainly in three different ways: **procedurally**, **manually** by a digital artist or through **3D acquisitions** of real objects and surfaces.
Let's explore each a little.

**Procedurality** works as always: a digital artist sets the *parameters* of an **algorithm** that then generates the mesh.
This has the usual pros and cons: on one hand the mesh *can be created at runtime* and *adapted* to the context it is needed in, allowing for *infinite variations*, but on the other hand we have much *less artistic control* over the results.

{% include responsive-image.html path="assets/images/09_ProceduralMeshes.png" alt="Example of procedural meshes" max-height="150px" %}

**Manual modelling** is much more interesting as it inserts itself in an *asset creation pipeline* sometimes called the **authoring pipeline**: using as reference the 2D sketches created by a concept artist, the 3D modeller produces a low-poly mesh of the object.
When doing so several different techniques can be used, including but not limited to the following two:

- **Low-poly modelling**: starting from a very simple mesh (*eg. a cube*) the artist manipulates its vertices and faces directly, creating new ones and moving them to where they need to be to create the desired mesh.
    By explicitly manipulating the structure of the mesh the artist can also ensure that the resulting model will have the exact *resolution* wanted.
    Usually this modelling step is done using quads as faces for the mesh since they're easier to work with: once the mesh is finished these faces will be easily transformed into triangles through a simple process called *triangulation*.

    <img src="/assets/images/09_LowPolyModelling.png" style="display: block; margin: auto; max-height: 150px" alt="Example of low-poly modelling">

    When creating meshes using this technique an extremely useful tool are what we commonly call **subdivision surfaces**, algorithms that operate on meshes to obtain *smoother, higher resolution meshes* by dividing existing faces into smaller ones which are then angled to create smoothness.
    By using these algorithms *iteratively* we can obtain smoother and smoother meshes at the cost of an increase in resolution: starting from a **control mesh** with a flat surface, the mesh is subdivided an ideally infinite number of times to obtain a **limit surface**.
    It goes without saying that the structure of the control mesh serves as the basis for the one of the limit mesh, and usually artists operate changes to the model between subdivision steps to obtain the desired surface.
    This process produces clean, regular meshes that are perfect for smooth, curved and organic objects.

    <img src="/assets/images/09_Subdivision.png" style="display: block; margin: auto; max-height: 150px" alt="Example of subdivision surfaces being iteratively applied">

    Many different subdivision schemas exist: *Doo-Sabin* (any polygonal mesh), *Loop* (triangle meshes only), *Butterfly* (triangle meshes only) and *Catmull-Clark* (any polygonal mesh, produces only quad meshes) just to cite some.
    The last one has recently gained popularity in video games where it is used *on the fly during rendering* to smoothen the surfaces of models.

- **Digital sculpting**: with this technique the artist completely ignores the underlying polygon structure of the mesh and instead **sculpts** it as if it was made of clay or another physical substance.
    By using their mouse as a chisel the artist gradually "removes substance" from a starting volume and thus defines the model: this mesh will usually be of *much higher resolution* since faces will automatically be created to reflect the more detailed sculpted shape.

{% include responsive-image.html path="assets/images/09_Sculpting.png" alt="Example of digital sculpting" max-height="150px" %}

These two techniques are pretty much antithetical to one another and they can be seen as the 3D counterparts of *pixel art* and *digital art* respectively: in the first case the artist is in charge of every single element of the design (vertices/pixels), while in the second they create the piece by broad strokes that don't care for the structure beneath them.
Because of the tight polygon budget we have in games we *usually prefer low-poly modelling* over digital sculpting.

The last way of creating models it through **3D acquisitions**, i.e. scanning a physical object with some specific equipment to obtain a mesh representation of it.
Many different techniques have been developed in this space: *laser scanners*, *time of flight systems*, *structured light* etc, all of which differ from one another based on the quality of the results (noise is pretty much a concern here), the invasiveness, the price and a bunch of other parameters.
The resulting models are usually akin to the ones obtained by digital sculpting in that they are far too complex for real-time applications.

{% include responsive-image.html path="assets/images/09_Scanning.png" alt="Example of 3D acquisition through scanning" max-height="150px" %}
