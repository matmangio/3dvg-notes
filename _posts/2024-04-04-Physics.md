---
title: Physics simulation
---

Physics can be thought of as a branch of **procedural animations** where objects move not through patterns designed by artists but according to the **laws of physics** or some approximation of them: these "animations" automatically *adapt to the context* and *add to the realism* of the scene, being naturally *repetition free*, but they of course offer *zero artistic control*.
The physics calculations are taken care of by the game's **physics engine**, a piece of software whose purpose is to **compute the transforms to apply in World space** to the object that need to be physically simulated: this simulation can be done in 3D or even 2D and has "soft" real-time requirements, meaning it has to be done efficiently but if it gets something wrong it isn't a big deal.
More specifically, good physics engines have these 3 characteristics:

- **Efficiency**: since we need to compute a new physics step (*almost*) at every frame, we can't have these computations take too long.
    In particular, the sweet spot for physics calculations is for them to take up to 5% - 30% of the maximum computation time, which for a game that runs at 30 FPS is 33 milliseconds in total.
    This need for speed has meant in the past that specific hardware for physics simulation was invented: nowadays, however, the most common approach is to parallelize these calculations on the GPU using the GP-GPU paradigm.

- **Plausibility**: since it is trying to simulate the laws of physics that regulate the real world, a good physics engine should produce plausible results.
    This doesn't mean that a physics engine should always be 100% *accurate*, however: it does not need to simulate the intricate physics laws of the real world and their interaction, but only a reasonable approximation of them.

- **Robustness**: no matter the circumstances, a good physics engine should never reach a state where things stop making any sense.
    This means working relatively well when dealing with both massive or miniscule forces and being able to correct things when they get too wrong: it is tolerable to have inconsistencies for a few frames, as long as they are corrected in the subsequent ones.

Note that physics engines are not only meant to add to the realism of the scenes: instead, many games use physics as a gameplay mechanic.
In those cases a good physics simulation is even more important!
Since it's a complex task, many game engines use external physic engines or libraries: the most known are Havoc by Microsoft, which mostly works on CPU, PhysX by Nvidia, which takes advantage of both CPU and GPU, and bullet, a library for OpenGL which as hardware accelerated and works on CPU.

{% include responsive-image.html path="assets/images/06a_PhysicsGameplay.png" alt="Examples of physics simulations being used for gameplay purposes" caption="Examples of physics simulations being used for gameplay purposes" %}

But which *kind* of physics is simulated by physics engines?
To recap, traditional mechanical physics is mainly divided in **three large fields of study**:

- **Dynamics**: how forces and impulses make objects move.
- **Statics**: how objects reach equilibrium states, states of minimal energy.
- **Kinematics**: how things actually move, regardless of what set them in motion.

Although they dabble in Kinematics, physics engines mostly work on **Dynamics** for a variety of objects: *Particles*, i.e. objects without volume; *Rigidbodies*, which are non-deformable rigid objects; *Articulated bodies*, i.e. objects with joints and constraints that regulate the movement of their parts; *Soft bodies* like cloth, ropes, hair and other deformable objects; *Fluids*, although this is particularly expensive and only done when strictly necessary.
On top of this, physics engine need to handle **Collisions**: they both need to detect them when they happen (*Collision detection*) and generate forces and impulses so that the objects respond to the collision in a believable way (*Collision response*).

## Basics of Newtonian Dynamics

Let's start by a useful clarification: the Scene Graph, the hierarchy of objects and the distinctions between Local and Global spaces are an abstraction that helps us work with 3D scenes, but they're *NOT taken into consideration by the Physics engine*.
In fact, **physics happens entirely in World space** and it doesn't take into consideration spatial relationships between objects if they are defined by their position in the hierarchy and not by a physical constraint.
The physics engine simply computes **changes to objects' state** (*position, orientation, ...*) in Global space: if we want to store these in a Scene Graph we'll have to translate them to changes in Local transforms as we've seen before.

Let's now review some concepts of Newtonian physics starting with the most common type of object: Rigidbodies.
The **state** of a rigidbody is defined by two parameters:

- its **position** in the 3D world, represented by a 3D vector;
- its **orientation**, represented by one of the numerous rotation representations (*absent with Particles*).

These spatial attributes can change over time at different rates and as the result of different entities.
In particular, we define the following:

- For the object's *position* $$p=(x,y,z)$$...
    - the **velocity** $$\vec{v}$$ represents the *rate of change of the position over time*, and is actually defined as the *derivative of the change in the position $$\Delta{p}$$ over time*:

        $$\vec{v} = \dot{p} = \lim_{t\rightarrow0} \frac{\Delta{p}}{t}$$

        When we talk of **speed** instead we usually intend the norm of the velocity vector $$\|\vec{v}\|$$, i.e. how "fast" the object is moving regardless of the direction it is moving in.

    - by multiplying velocity by the object's mass we obtain **momentum**, a measure of much the object *wants to keep its velocity*: it ties in with the concept of **inertia**, where each object not subject to forces *keeps its state of motion* and this state is only allowed to change *continuously*.

        $$\vec{\rho} = m\vec{v}$$

        The **mass** $$m$$ of an object is a measure of its resistance to changes in the velocity: that is why it is also sometimes called *inertial mass*.
        Incidentally, this mass is also equal to the *gravitational mass*, which is a measure of the ability to attract other objects.

    - we can also define the *rate of change of the velocity*, which is called **acceleration**: it too is the *derivative of velocity over time*, so it is actually the *second derivative of position over time*.

        $$\vec{a} = \dot{\vec{v}} = \ddot{p}$$

    - **forces** are what produce change in the velocity of an object and they're defined as *mass times the acceleration* they impose: it's easy to see that forces are the *derivative of momentum*.

        $$\vec{f} = m\vec{a}$$

        Newton's third law of dynamics tells us that *whenever there is a force acting on an object inside a system there is also an opposite one acting on a different object*, so if we summed up all forces in a system we would always get $$0$$.
        The force being the derivative of momentum then means that since the sum of all momentum on the objects is zero, then **the total momentum in a system is kept constant** throughout time.

        $$\sum{\vec{f}} = 0 \; \rightarrow \sum{\vec{\rho}} = k \;$$

- For the object's *rotation* $$r$$...
    - the **angular velocity** $$\vec{\omega}$$ is the *rate of change of the orientation over time*, which can look very different depending on the representation chosen for rotations.

    - we can then obtain the **angular momentum** by scaling the angular velocity by the **moment of inertia** $$I$$, a measure of how much the object is resistant to change in its orientation that depends on the mass of the object and its *distribution in the shape*: in particular its value is large when the mass is mostly on the extremities of the object and low when it is concentrated prevalently around its barycenter.

        $$L = I\vec{\omega}$$

        In 3D, the value for $$I$$ depends on the axis of rotation, which is why $$I$$ is usually computed from a *3x3 matrix $$M$$ expressing the mass distribution* called the **inertia tensor**.
        To obtain the moment of inertia for axis $$\hat{a}$$ we can in fact compute the following:

        $$I = \hat{a}^T \cdot M \cdot \hat{a}$$

    - similarly to what happened for position, we can also define the *rate of change of angular velocity over time* as **angular acceleration** $$\vec{\alpha}$$, which is then the 2nd derivative of the change of rotation.

    - the equivalent of forces for rotation is **torque**, which can be defined as the *product of the moment of inertia and the angular acceleration* they impose over the object:

        $$\vec{\tau} = I \vec{\alpha}$$

Now, between these parameters is most important that we identify what is the **state** of the object, i.e. what is **kept** throughout time and that the object is resilient to lose thanks to inertia: in particular, *position, velocity, orientation and angular velocity* are all the factors that constitute the **state of the object** (*and also momentum and angular momentum*), while the two *accelerations* are **changes to the state** that the object doesn't try to keep (*and also force and torque*).
The role of the physics engine is then to **update the state** of all objects at each frame in a way that reflects these physics laws.

{% include responsive-image.html path="assets/images/06a_Parameters.png" alt="Table of the different parameters" max-width="450px" %}

As we've seen in the formulas above, each rigidbody also has a few quantities associated with it: these are **constants** that won't normally change for the object and that determine how it reacts to forces and other solicitations.
These are the **mass**, **moment of inertia**, **drag** and **barycenter**: of these, the only ones we haven't talked about yet are the barycenter, also known as the *center of mass*, and drag, which we'll cover in later paragraphs.
For now, let's quickly define the barycenter of an object. \\
The idiom "center of mass" explains well what the barycenter of an object is: it is a *fixed position* that is the *weighted average of the position of the sub-parts of the object*, each weighted by their relative mass.
It is such an important point since **objects rotate around their barycenter**: this is why *in physics simulation the position of a rigidbody is better described as the position of its barycenter*.

## Particle dynamics

Let's start our discussion of proper physics simulation by examining the simpler case, the one for **particles**: as we said before, these are ideal point-like objects that don't have a shape and have all of their mass concentrated in one point.
Thanks to these characteristics we can begin by *ignoring all parameters concerning rotation*: everything we develop here will also apply to rigidbodies, however, as the treatment of position and the related attributes is the same.
We'll also see later how some game engines get away with only simulating particles and get back rigidbody behaviour with some tricks.

So, a particle's state is defined by its *mass*, its *drag* factor, its *position* $$p(t)$$ at time $$t$$ and its *velocity* $$\vec{v}(t)$$ at time $$t$$.
It is useful to underline from the beginning that this time $$t$$ is not real world time but instead a **simulated time**: sometimes this flows in sync with what is also called *clock time*, but there are situations where this could not be true (e.g. when the game is paused, t is constant).

### Analytical solutions

Now, from an **analytical** standpoint, since physics is deterministic we could identify the forces that will act on the particle at time $$t$$ based on the position of the particle itself: this argument is needed since complex forces like the electromagnetic or gravitational ones depend on the particle's position and other factors.
We can thus imagine having a *function that returns the resulting force on the particle at time $$t$$*:

$$\vec{f}(t) = function(p(t), \dots)$$

Dividing this total force by the particle's mass we get the acceleration for that frame:

$$\vec{a}(t) = \frac{\vec{f}(t)}{m}$$

This in turn allows us to analytically determine the velocity and position of the particle at time $$t$$:

$$
\vec{v}(t) = \vec{v}_0 + \int_{t' = 0}^{t} \vec{a}(t') \cdot dt'\\
p(t) = p_0 + \int_{t' = 0}^{t} \vec{v}(t') \cdot dt'
$$

where $$p_0$$ and $$\vec{v}_0$$ represent the initial position and velocity at time $$t=0$$, which need to be given.
We can therefore recap this whole thing in a single system that we could **solve for $$t$$** in order to get the position of the particle at each time $$t$$ and thus update its global transform accordingly:

$$
\begin{cases}
    \vec{f}(t) = function(p(t), \dots) \\
    \vec{a}(t) = \frac{\vec{f}(t)}{m} \\
    \vec{v}(t) = \vec{v}_0 + \int_{t' = 0}^{t} \vec{a}(t') \cdot dt'\\
    p(t) = p_0 + \int_{t' = 0}^{t} \vec{v}(t') \cdot dt'
\end{cases}
$$

Since integrals are hellish to compute, physicists write this system equivalently using derivatives:

$$
\begin{cases}
    \vec{f}(t) = function(p(t), \dots) \\
    \vec{a}(t) = \ddot{p}(t) = \frac{\vec{f}(t)}{m} \\
    \vec{v}(t) = \dot{p}(t) \\
    \dot{p}(0) = \vec{v}_0 \\
    p(0) = p_0
\end{cases}
$$

This analytical solution, however, **isn't usually suitable for real-time video games** for a series of reasons.
First of all it is a system of *Ordinary Differential Equations*, which is not something we generally want to solve at runtime for each particle that composes a scene.
Secondly, it is **circular**: *the position affects the force, which affects the acceleration, which affects the velocity, which affects the position*.
This makes it even more difficult to solve accurately: most of the times, an analytical solution **doesn't exist** in a form that we can compute using common algebraic functions.

When the solution does exist, however, it is **extremely convenient**: it allows us to find the position and velocity of the particle at each point in time, even predicting the future or going back in the past.
An example of a simple system like this the ballistic of a single bullet subject only to gravity: in that case, if we know $$p_0$$ and $$\vec{v}_0$$ we can predict the whole parabola of the bullet since the *force being constant* allows us to break the circular dependency and get simple equations for velocity and position.

$$\text{Given } \vec{f} = \begin{pmatrix}0 \\ -9.8 \end{pmatrix} \; \rightarrow \; \begin{cases} \vec{f} = \begin{pmatrix}0 \\ -9.8 \end{pmatrix} \\ \vec{a} = \frac{\vec{f}}{m} \\ \vec{v}(t) = \vec{v}_0 + \int_{t' = 0}^{t} \vec{a}(t') \cdot dt' = \vec{v}_0 + \vec{a}t \\ p(t) = p_0 + \int_{t' = 0}^{t} \vec{v}(t') \cdot dt' = p_0 + \vec{v}_0t + \frac{1}{2}\vec{a}t^2 \end{cases}$$

{% include responsive-image.html path="assets/images/06a_AnalyticalSolution.png" alt="Example of parabolic bullet trajectory" max-width="200px" %}

### Numerical solutions

Instead of exactly solving the integrals, the idea of **numerical solutions** is to **quantize time** in intervals of a certain length and compute the state of the particle at discrete time steps using the state at the previous step as an input.

```bash
1. state(t = 0) <- init
2. state(t + 1) <- physics_step(state(t))
3. goto 2
```

{% include responsive-image.html path="assets/images/06a_NumericalSolution.png" alt="Example of the same parabolic bullet trajectory but with numerical step-by-step solutions" max-width="230px" %}

More practically, we use **numerical integrators** to approximate the calculation of the integrals in the system above: these algorithms compute them as the *summed area of small rectangles*.
For the physics engine this means updating the position and velocity of the particle at each **physics step**: between one step and another we **assume forces to be constant** so that the numerical integrator can analytically compute the new state.
By this description we can easily guess that the choice of the *duration of physics steps* $${\bf dt}$$, also known as **physics delta time** (*i.e. the width of the rectangle*), is of outmost importance: the more physics steps we have, the more accurate the system is but the more expensive it becomes.
Note that the physics delta time *doesn't need to match the time between rendering frames* and it also *doesn't need to be constant*, although it typically is: a common solution is to have at least as many (or preferably twice as many) physics steps as rendering steps.

{% include responsive-image.html path="assets/images/06a_PhysicsSteps.png" alt="Visualization of physics steps with regards to rendering frames" max-width="400px" %}

In general, numerical integrators offer many advantages over analytical ones: for once, they are **generic**, meaning they work with every possible scenario and don't need to be found by solving a difficult system of equations.
They are, however, **more expensive** since they require iterations to work, and they are prone to **integration errors** that we'll need to account for.

Let's talk a bit of these integration errors: because the numerical systems only approximate the value of integrals, it is inevitable that they get some things wrong.
This **simulation error** accumulates over simulation steps and the magnitude of it is dependent on the choice of $$dt$$: the smaller this value is, the more accurate but expensive the simulation will be.
Also, numerical integrators are quite sensitive to *sudden changes in the forces*: if the derivative of the force graph is very steep things break quickly, since the assumption that force is constant in a step is no longer valid; we'll see how we can fix this with impulses.
Anyway, something we can evaluate the numerical integrators with is then the **order of convergence** of the error as $$dt$$ changes (*also known as the **order of simulation***), meaning we're asking ourselves the question *"how much does the total error decrease as $$dt$$ decreases?"*

An often overlooked effect of integration errors is that they affect the **total energy** of the system.
In a real system the total energy can *never increase* and in fact it often decreases over time as an effect of dissipations, that is *attrition* turns energy into heat.
In our integration systems, however, it may well be that the total energy of a system increases over time: this is bad both in terms of *stability*, since with bigger and bigger velocities the integration errors become bigger and bigger as well, and in terms of *plausibility*, since as humans we can sense something is wrong.
A very simple solution to this is to always make sure that the simulation includes **attritions** with friction forces, making it more stable and robust.

We now explore a number of different numerical integrators that have totally different approaches to computing the particles' state.
We will then evaluate them based on their:

- *efficiency*, which must be at least soft real-time;
- *accuracy*, where we care that results appear plausible (even if different from reality);
- *robustness*, where we want rare wrong results and NO crashes;
- *generality*, i.e. which situations they are capable of recreating and under which constraints.

#### Euler integration

A first numerical approach is called **euler integration** and it works as follows.
Given a certain initial position $$p$$ and an initial velocity $$\vec{v}$$, at each physics step we:

1. Compute the total **force** acting on the particle as a **function of current positions** of it or other particles and any other parameter that may be needed.

2. Compute the **acceleration** by dividing this total force by the particle's mass.

3. Update the **position** using the **current velocity** multiplied by the physics delta time.

4. Update the **velocity** using the **acceleration** multiplied by the physics delta time.

5. Update the current time by summing the physics delta time and loop.

{% include responsive-image.html path="assets/images/06a_EulerIntegration.png" alt="Visual recap of the euler integration system" max-width="250px" %}

From this schema it is very apparent that position and velocity constitute the state of the particle, while force and acceleration are computed at each step but not maintained between them.
Also note that the position is updated with the velocity computed in the **last step**, since we assume that the new velocity is only available after the forces have acted on the particle for the duration of this step: this particular choice gets this integration method the name of **Forward euler integration**.

What about the integration error with this system?
It can be demonstrated that with Forward euler the order of convergence of the cumulated error is $$O(dt^1)$$, so **linear** in $$dt$$ (*the error each frame is quadratic in $$dt$$*): this is not good, since doubling the number of simulation steps (and thus the computational requirements) would only halve the total error.
Moreover, this method becomes really **unstable with large $$dt$$**, which is not a good sign, and has a **no reversibility**, meaning we cannot reverse time as an analytical system would, not even in theory: if we needed such an effect, with Forward euler we would need to store states in a table.

#### Symplectic euler

There exists a variant of the Forward euler integration system called **Symplectic euler**: it works exactly as the Forward euler did, with the only exception that **velocity is updated before position**.
This means that the object travels in this step using *next step's velocity*, which is theoretically kind of wrong but has been recorded to **work better in practice**.

{% include responsive-image.html path="assets/images/06a_SymplecticEuler.png" alt="Visual recap of the symplectic euler integration system" max-width="250px" %}

In particular, although the *order of convergence of the error doesn't change*, we get a better behaviour on average: the method is **more stable** and surprisingly even **more accurate**.

#### Leapfrog integration

The **leapfrog integration method** stems from a basic idea: up until now we've updated the position and velocity at the same time $$t$$, but what if that wasn't the case?
In particular, this new integration method stores **positions at time $$k \cdot dt$$** but **velocities at time $$(k+\frac{1}{2})dt$$** for each particle, that is the velocity is sampled **half a step forward** (*from which the name "leapfrog"*).
This may come as a surprise, since it doesn't appear to make sense: it can be demonstrated, though, that this is the equivalent of using the sum of *trapezoids* of base $$dt$$ and height $$v(t)$$ to approximate the integrals instead of simple rectangles, which naturally provide a better approximation.

From the code point of view this new method is an easy implementation, since it doesn't require a drastic change: instead, only the initialization is affected, where the initial velocity is computed using *half of the acceleration in the starting position*.
Subsequent steps update position and velocity as normal, first updating the position and then the velocity: notice how the velocities are updated with the acceleration the particles are subjected to "half a step before".

$$
\begin{align*}
\text{Initialization: } \quad & p_0 \text{ is given}, &\vec{v}_{0.5} = \vec{v}_0 + \vec{a} \frac{dt}{2} \\
\text{Subsequent steps: } \quad & p_{i+1} = p_i + \vec{v}_{i+\frac{1}{2}}dt, &\vec{v}_{i+\frac{3}{2}} = \vec{\vec{v}_{i+\frac{1}{2}}} + \vec{a} \cdot dt
\end{align*}
$$

This simple change in meaning that only requires a small adjustment of the initialization proves extremely impactful on accuracy: now the cumulated error goes down **quadratically** as $$dt$$ decreases (*the error each frame is proportional to $$dt^3$$*), which is quite the improvement!
However, for it to work the leapfrog method requires **constant $$dt$$** during all the simulation, which most times is a cost we will be willing to pay for a simple integration method that greatly reduces the total error and is also *fully reversible* (although only in theory, since numerical errors often make going back in time quite hard).

### Forces

Let's stop our exploration of the different integration methods to talk a bit about **forces**.
In both euler integration systems we've seen that we have a function that given the *position of particles and any other parameter* returns the total force acting on a particle.
This total force is most often computed as the **vector sum** of all the different singular forces acting on the particle: we now see a number of these simple forces and how they're computed.
Common forces seen in games are:

- **Gravitational forces**: there are two kinds of gravitational forces depending on the scenario.
    Usually we use the so called *gravity* force, which is used when the object is on the surface of a planet:

    $$\vec{f} = mg\hat{d}_{Down}$$

    where $$m$$ is the mass of the particle, $$g$$ is a *gravitational constant* (9.81 for earth) and $$\hat{d}_{Down}$$ is the world's downward direction.
    Notice how this force doesn't depend on the position of the particle, since we assume that the planet is so much bigger than the particle that the distance between them can largely be considered constant.

    The other kind of gravitational force is instead used in *open space*: here the relative positions and mass of the two objects matter, as the gravitational pull that tries to bring them closer is stronger the closer the two particles are to each other.
    If we call $$p_a$$ the position of particle A and $$p_b$$ the position of particle B, then the force to which A is subject is *directed towards B* and the formula is:

    $$
    \begin{align*}
    \vec{f}_a &= \frac{G m_a m_b}{\| p_b - p_a \|^2} \frac{p_b - p_a}{\| p_b - p_a\|} = \frac{G m_a m_b}{\| p_b - p_a \|^3} (p_b - p_a) \\
    \vec{f}_b &= -\vec{f}_a
    \end{align*}
    $$

    where $$G$$ is an universal constant.
    Notice how the force is proportional to the masses and inverse proportional to the distance between the particles.

- **Electric forces**: these are either attractive or repulsive forces based on the sign of the particle charges, where particles with the same sign charge will repel each other while if the sign is different they will attract each other.
    The force is similar to the gravitational one and if we call $$q_a$$ the charge of particle A and $$q_b$$ the charge of particle B the formula is:

    $$
    \begin{align*}
    \vec{f_a} &= \frac{-K q_a q_b}{\| p_b - p_a \|^2} \frac{p_b - p_a}{\| p_b - p_a \|} = \frac{-K q_a q_b}{\| p_b - p_a \|^3} (p_b - p_a) \\
    \vec{f_b} &= -\vec{f_a}
    \end{align*}
    $$

    where $$K$$ is also an universal constant.

- **Wind pressure**: this is a force acting on surfaces, for example a boat sails.
    In particular, the more the surface is orthogonal to the direction of the wind $$\vec{w}$$ the stronger the force is: this orthogonality is computed as the dot product of the area vector with the wind direction.
    If we call $$p_0, p_1, p_2$$ three vertices of a triangle, the wind force acting on it is:

    $$\vec{f} = \left|\left| \left( \frac{1}{2} (p_1 - p_0) \times (p_2 - p_0)\right) \cdot \vec{w} \right|\right| \frac{\vec{w}}{\|\vec{w}\|}$$

    To obtain the force acting on the single vertices of the triangle we simply divide it by $$3$$.

- **Buoyancy**: this force acts on the objects immersed in water or other liquids, has direction opposite to the gravitational force and modulo equal to the *weight of the displaced liquid*.
    It is the force that makes certain objects float and other sink.

There are then other kinds of forces that merit their own paragraph due to their particular use or implications: these are elastic forces, control forces and friction forces.

#### Elastic forces

Elastic forces are one of the most common forces in video games: **springs** are basically just physical constraints that *counter the change of distance between two particles* positioned at the ends of the spring itself.
Each spring is characterized by a *rest length $$l$$*, that is the length it has when not subjected to any force, and a *stiffness $$k$$* that represents the strength with which the spring opposes expansion and compression.
**Hooke's Law** then gives us a formula to express elastic forces based on these parameters and the positions $$p_A$$ and $$p_B$$ of particles $$A$$ and $$B$$ at either end of the spring:

$$
\begin{align*}
\vec{f}_a &= k(\|p_b - p_a\| - l) \frac{p_b - p_a}{\|p_b - p_a\|} \\
\vec{f}_b &= -\vec{f}_a
\end{align*}
$$

These elastic forces are used in games by creating "fake" springs between particles that we want to keep at the *same distance* but who are allowed to move a little: these configurations are sometimes called **Mass-and-Spring systems** and are very useful to simulate **elastic deformable objects** (*aka soft bodies*) like cloth, hair, ropes etc, which is objects that can deform but tend to return to their original shape.

{% include responsive-image.html path="assets/images/06a_MassSpring.png" alt="Example of a Mass-and-Spring system" caption="Example of a Mass-and-Spring system where the grey springs are used to model resistance to bending" max-width="200px"%}

Mass-and-Spring systems could also be used to model *plastic deformable objects*, that is objects that can be deformed and keep their deformed shape: this could be done by dynamically changing the rest length of the springs in response to extreme stretching or compressions, however this is not so easy.
What these systems *cannot* be used for is to simulate Rigidbodies or inextensible ropes: although we could raise the stiffness of each spring to absurd amounts and in theory this would work, using very large numbers leads to numerical instability and would require unfeasibly small values of $$dt$$ to work.

#### Control forces

The term **control forces** identifies all those forces that *don't have a physical meaning but are required for gameplay*: let's imagine for example a forward force that is applied to a player character when the forward button is pressed.
This force is not justified in any way to the physics system, it is simply applied to the object because we need to give the player a way to control the character.

Although they're not really frowned upon, many game developers agree that it's better when a game doesn't use a lot of control forces: *the more physically justified the forces, the better*.
Using only proper forces improves the realism of the game world but is however much harder to control, so a balance between the two approaches is usually the best solution.

#### Friction and drag

Another category of forces that is fundamental in the real world is **attrition forces**, sometimes called **friction**, which are *forces that oppose motion*.
In particular, there are two kinds of attrition forces:

- **Isotropic friction forces**, which oppose movement *regardless of its direction*: they model the effect of the medium through which the object moves, like for example air or water.
    These forces always have *direction opposite to the movement*, and their *magnitude depends on the speed of the object* instead of its position: the faster an object moves, the more isotropic friction is applied to it.

    $$\vec{f}_f = -k\vec{v}$$

- **Planar friction forces**, which are applied when *things slide against each other*, like for example a box being dragged on the ground.
    In this case the direction is always *parallel to the contact plane* and the *magnitude depends on the force that the object applies to the contact plane in the orthogonal direction* (eg. its weight if the plane is horizontal).

These friction forces are the ones mainly responsible for the dissipation of energy in a system, which is something we've seen numerical integrators struggle with: it is often the case that, if left unchecked, numerical integrators may even increase the total energy in a system, completely breaking plausibility.
A solution to this problem is to compute these friction forces and add them in the total force computation: unfortunately this is quite hard work and may also introduce even more integration errors.

A better solution is to use a technique called **velocity damping** that simulates isotropic friction: instead of computing the friction force on all moving objects we simply **reduce all velocity vectors by a fixed proportion**, basically applying a fixed percentage "tax" on speed by a factor usually called **drag**.
The basic idea of this is that the higher the speed, the more attrition should be applied and thus the more speed should be lost: a proportional decrease of speed achieves exactly this!
Most of the time drag is defined per-second (*eg. "2% reduction of speed every second"*), so we need the physics delta time $$dt$$ to apply this drag at every physics step:

$$\vec{v} \leftarrow (1 - drag)^{dt} \vec{v}$$

Since exponentials are hard to compute, especially with small exponents, this is usually approximated to:

$$\vec{v} \leftarrow (1 - drag \cdot dt) \vec{v}$$

which is a good approximation if $$drag \cdot dt$$ is sufficiently small, which it is most of the time.
Note that with this method we can also simulate *planar friction* by *splitting velocity into orthogonal/parallel components and applying different frictions* to each.

Velocity damping is an extremely useful tool since it prevents the total energy in the system from ever increasing and makes the simulation more robust.
On the other side it may also exaggerate the effect of frictions, especially in the absence of collisions between objects: in this regard, the choice of the drag factor is extremely important.
In practice, a low drag factor is hardly noticeable and increases plausibility, but as it increases objects start to quickly grind to a halt more and more rapidly to a point where inertia doesn't exist anymore.

### Impulses and kinematic displacements

We've discussed before that the state of a particle/rigidbody, that is its position and velocity, can *only change continuously*, with no sudden jumps: this makes sense since in real life objects cannot teleport but instead have to move between positions, and they cannot instantly speed up or come to a halt if not for the effect of very big accelerations.
In our physics simulation, however, we may sometimes want to **break this assumption** for specific purposes: let's see what effects we achieve.

If we instantly change the position of an object we get what is called **kinematic displacement**: the object simply teleports to its new position and keeps its velocity.
This is achieved by simply adding a displacement factor $$dp$$ to the object's position:

$$p \leftarrow p + dp$$

Displacements of this type are most useful in video games, where we may for example need to instantly teleport the player character or some other objects for gameplay purposes.

The need to suddenly change velocity is not as apparent but it has to with **very intense but short forces**, as is the case for **impacts**.
Let's consider for example a ball bouncing on the ground: in this case we have a very strong force applied for a very short time.
The problem here is that our integration systems cannot handle this kind of force well, since they need to assume that between simulation steps forces are constant and impacts such as these very much destroy this assumption.

{% include responsive-image.html path="assets/images/06a_Impact.png" alt="Visualization of an impact in the physics steps timeline" max-width="400px"%}

The solution in this case are **impulses**, that is discontinuous changes to the velocity of the object that simulate forces whose application time approaches zero and whose intensity approaches infinity.
They're often expressed in the form of a vector $$\vec{i}$$ that represents the force already multiplied by the time: this way the immense force is countered by the miniscule time and the physics engine doesn't need to handle either extremely large or extremely small numbers, both of which would create numeric instability.
That said, we can apply impulses as such:

$$\vec{v} \leftarrow \vec{v} + \frac{\vec{i}}{m}$$

This causes a sudden change in velocity that, although not physically correct, is useful to **model phenomena with a time scale much shorter than $$dt$$**.
As we will see, because of this interpretation impulses will be used in collision response to make objects bounce off of one another.

## Verlet integration

Up until now we've considered the state of a particle as being composed by velocity and position, but a branch of physics simulation rejects this assumption: instead, they record the state of a particle by storing the **current position and the position in the previous step**.
This way *velocity at the previous step is stored implicitly*, since it is just the vector between positions scaled by the inverse of $$dt$$:

$$
\begin{align*}
\text{Previously: } \quad & p_{now} = p_{old} + \vec{v}dt \\
\text{Now: } \quad & \vec{v} = \frac{p_{now} - p_{old}}{dt}
\end{align*}
$$

The integration method that uses this new definition of state is called **Verlet integration**: it initializes the current and previous positions to set the particles starting position and velocity following the rule above.
It then works exactly like euler integration in that each step it computes the acceleration and updates the position, although here to update the velocity we simply update the variable holding the previous position to the current one.
Note how we don't even need to actually compute the velocity, we can just use it implicitly in creating the next position the object will occupy:

$$
p_{next} = p_{now} + (\vec{v} + \vec{a} \cdot dt)dt = p_{now} + \left( \frac{p_{now} - p_{old}}{dt} + \vec{a} \cdot dt \right) = 2p_{now} - p_{old} + \vec{a} \cdot {dt}^2
$$

As is now apparent from this formula, in absence of acceleration the next position the object will occupy is simply the **extrapolation** of $$p_{now}$$ and $$p_{old}$$ with factor 2: geometrically speaking we continue moving on the same trajectory, which is physically correct.

{% include responsive-image.html path="assets/images/06a_VerletExtrapolation.png" alt="Geometric representation of Verlet integration in absence of acceleration" max-width="230px" %}

Everything else is kept the same from the implementation's point of view, save for the fact that we now need an auxiliary variable to store the newly computed position that will substitute the current one.
So in total, Verlet integration system's implementation looks a bit like this:

{% include responsive-image.html path="assets/images/06a_Verlet.png" alt="Visual representation of verlet integration" max-width="400px" %}

How does it fare against the other integration systems?
Although the velocity is now implicit, this doesn't save storage space (RAM) since there is now the need to store the old position: both those types of variables are represented by 3D vectors, so it is exactly the same.
Despite this missed opportunity, Verlet integration offers a good accuracy/efficiency ratio, being the same complexity as Euler methods but having accumulated error in the **second order** (*$${dt}^2$$, while the error per-step is linear in $$dt$$*).
This integration system also offers **total reversibility**, although as always this is easier said than done.

### Position Based Dynamics

But why choose the Verlet integration system over, for example, leapfrog integration?
Although it may not appear obvious at first, not storing the velocity explicitly offers many advantages: for one, we can change the position of an object directly and have these *kinematic changes reflect immediately on the velocity*!
Let's say, for example, that we want an object to immediately stop: while with other integration systems we would need to apply an adequate acceleration, with Verlet integration we can just make the old position equal to the current one and have the velocity be zero "by accident".

More generally, Verlet integration allows for the easy introduction of **positional constraints**, *equalities or inequalities that involve the positions of particles* and that are very useful to model *consistency conditions*.
Although these can be used with all integration systems, with Verlet integration we can enforce these constraints by *changing the soon-to-be position $$p_{next}$$ before it is applied* and have **these changes reflect on the velocity** of the particle automatically: this update of velocity is not necessarily correct, but it is often plausible and hugely contributes to the robustness of the system.
A system that makes heavy use of positional constraints is said to follow **Position Based Dynamics** (**PBD**).

{% include responsive-image.html path="assets/images/06a_PBD.png" alt="Visual representation of verlet integration with PBD" max-width="400px" %}

Let's say for example that we want all particles to be above ground, which in our example means that their $$y$$ coordinate is non-negative: with Verlet we can just cycle over the particles in the system and enforce this constraint by changing the position.

```c#
foreach (Particle p in particles) {
    if (p.pos.y < 0) p.pos.y = 0;
}
```

Similarly we can constrain the position of particles inside a box and ensure they cannot get out:

```c#
foreach (Particle p in particles) {
    p.pos.x = clamp(p.pos.x, 0, 100);
    p.pos.y = clamp(p.pos.y, 0, 100);
}
```

Imposing constraints like the ones we've just seen is the first step of *collision response*, a topic which is however out of the scope of this chapter and we'll tackle later down the line.
Also not that to make objects bounce over the planes we've defined in these examples we'll still need impulses.

Using Position Based Dynamics with Verlet integration greatly increases the **flexibility** of the system since it allows for the creation of arbitrary constraints that can model a plethora of different situations: these constraints are also very easy to define and impose, especially when they involve only a few particles (*eg. keep two particles at distance $$k$$*).
It also increases the **robustness** of the system, ensuring that we'll never see particles breaking our rules while not requiring impulses or forces to do so.
However, this paradigm shift opens the doors to a number of new questions:

- How do we actually *enforce* these positional constraints in practice and how do we *choose between the many different positions* that would satisfy the constraint?
- This paradigm **requires $$dt$$ to be fixed** to work properly: how do we *correct if $$dt$$ varies*?
- How do we *act on velocity directly*, for example for applying impulses?
- How do we *change the position without impacting velocity*, for example for teleports?
- How do we *apply velocity damps* to account for friction forces?

We will try to answer these questions in the following paragraphs and we'll also try to better explore how these positional constraints are expressed.

#### Dealing with variable $$dt$$

How do we account for variable delta time in Verlet integration?
We can in fact see that if $$dt$$ changes then the velocity of the particle changes erratically, which is something we definitely do not want.
To account for this, *whenever $$dt$$ changes we need to also change $$p_{old}$$ to keep the velocity constant*; in particular, if we call $$dt$$ the last value for the delta time and $$dt'$$ the new value, then we want:

$$
\frac{p_{now} - \color{blue}{p_{old}'}}{dt'} = \frac{p_{now} - p_{old}}{dt} \\
\color{blue}{p_{old}'} = p_{now}\frac{dt - dt'}{dt} + p_{old} \frac{dt'}{dt}
$$

#### Velocity damping in Verlet

For the same exact reasons as before we would want to simulate frictions with the Verlet integration system by multiplying the velocity by a factor of $$c_{damp}$$ each frame (*equal to $$(1-dt \cdot drag)$$*): however, while in other integration methods this could simply be done by changing the value for velocity, here we need to mess around a little bit more.

We could do this by changing the old position each frame, thus adjusting the inferred velocity, but there's actually a simpler way to do it.
In particular, what we can do is **change the extrapolation formula** for $$p_{next}$$ to include velocity damping as the extrapolation factor:

$$
p_{next} = \color{blue}{(1 + c_{damp})} \cdot p_{now} - \color{blue}{c_{damp}} \cdot p_{old} + \vec{a} \cdot {dt}^2 \\
p_{next} = \color{blue}{(2 - dt \cdot drag)} \cdot p_{now} - \color{blue}{(dt \cdot drag - 1)} \cdot p_{old} + \vec{a} \cdot {dt}^2
$$

As we can see, with both equivalent formulas the coefficients sum up to one, thus maintaining the correctness of the extrapolation: only now we extrapolate with a factor $$\leq 2$$ to simulate attritions.

#### Acting on velocity and position

Let's now discuss how we can directly and *instantly change velocity or position* when using Verlet, for example for applying impulses or teleport a particle.
Let's start with velocity: if we want the velocity to be changed of a certain $$d\vec{v}$$, which could for example be an impulse $$\vec{i}$$ divided by the mass of the particle, we need to **update the old position** so that the inferred velocity is the new one.
This can be done as such:

$$
\vec{v}' = \vec{v} + d\vec{v} \\
\frac{p_{now} - \color{blue}{p_{old}'}}{dt} = \frac{p_{now} - p_{old}}{dt} + d\vec{v} \\
\color{blue}{p_{old}'} = p_{old} - d\vec{v} \cdot dt
$$

Similarly, if we want to teleport a particle while maintaining its velocity we not only need to update the current position $$p_{now}$$ but also the previous position $$p_{old}$$ as such:

$$
\vec{v}' = vec{v} \\
\frac{p_{now}' - \color{blue}{p_{old}'}}{dt} = \frac{p_{now} - p_{old}}{dt} \\
\color{blue}{p_{old}'} = p_{now}' - (p_{now} - p_{old})
$$

### Enforcing constraints

To understand how positional constraints are defined and enforced let's start with an example, one of the most common and simple positional constraint: the **equidistance constraints**, which basically asks for two particles $$A$$ and $$B$$ to be kept at a constant distance $$d$$.
This resembles a spring in many ways, except it isn't a force but rather a constraint that is applied instantly during the constraint enforcement phase at the end of each physics step: rather than a deformable elastic spring, an equidistance constraint models a *rigid rod* between the two objects, which is why positional constraints are sometimes called *"hard" constraints* as opposed to force-based ones like strings which are *"soft" constraints*.

$$
\| p_A - p_B \| = d
$$

Let's assume for now that the particles' mass is the same: in that case, the best way to enforce the constraint is to *move each particle by the same amount* which equals to half the "error" amount.
We can write this in a way that is agnostic of whether the two particles are closer or further away than $$d$$:

$$
\begin{align*}
\vec{v}_A &= \frac{1}{2}(d - \| p_A - p_B\|)\frac{p_A - p_B}{\| p_A - p_B\|} \\
\vec{v}_B &= -\vec{v}_A
\end{align*}
$$

#### Equality and inequality constraints

Before seeing how we can enforce multiple constraints at a time we first need to define and categorize positional constraints.
In general, positional constraints are **predicates defined on the positions of a number of particles**, usually between 1 and 4:

$$\mathcal{C} : (p_1, p_2, \dots, p_n) \rightarrow \{true, false\}$$

These predicates can be defined as either **equalities** or **inequalities**.
Some examples of equality constraints can be the equidistance constraint defined earlier (*"these two particles must remain at distance $$k$$"*), fixed positions (*"this particle must stay in position $$p$$"*), coplanarity/collinearity ("*these particles need to stay on the same line/plane*"), volume preservations and so much more. \\
Inequality constraints are a bit more interesting in that they are **enforced as equality constraints only when they're broken**.
Let's consider for example the *"please don't sink below ground"* we've talked about before, which can be expressed as $$\mathcal{C}(p_a) \leftrightarrow p_a.y \geq 0$$: when this constraint isn't satisfied what the solver needs to do is simply enforce the equality constraint $$\mathcal{C}(p_a) \leftrightarrow p_a.y = 0$$, placing the object exactly on the ground.
That said, other kinds of inequality constraints include minimum distance (*"these two particles must be at least this far apart"*), which are used for collision handling, maximum distance (*"these two particles must be at most this far apart"*), which are used for inextensible ropes, angle constraints (*"the angle that these two particles form with this point should be at most this"*), which are used for joints, and many more.

So how do we enforce constraints?
As we've seen, this question boils down to *how do we enforce equality constraints* since inequality constraints are applied as such when broken.
Well, the basic principle when enforcing positional constraints is to **minimize the sum of squared displacements** of the particles with regards to their current position: this way we satisfy the constraint while changing the system the least possible amount.
The use of the squared displacement is purposeful: it incentivizes a number of smaller displacements rather than a single big displacement, which usually looks much less plausible.
To satisfy each kind of constraint *the minimizer must be found analytically* like we did for the equidistance constraint; however, each of them must find the displacements $$\vec{d}_1, \vec{d}_2, \vec{d}_3, \dots$$ that are:

$$
\begin{align*}
\text{argmin}_{\vec{d}_1, \vec{d}_2, \dots}(\|\vec{d}_1\|^2 + \|\vec{d}_2\|^2 + \|\vec{d}_3\|^2 + \dots) & \\
{\small \text{such that }} \; \mathcal{C}(p_1 + \vec{d}_1, p_2 + \vec{d}_2, p_3 + \vec{d}_3, \dots) &
\end{align*}
$$

In practice, however, it is not only the length of the displacements $$\vec{d}_i$$ that matters, but also the mass of the particles to which they are applied: ideally we would like to move heavy particles less and light particles more, since mass is a measure of objects not wanting to move.
What we're really trying to do is then to **minimize the change to the total momentum** of the system:

$$
\begin{align*}
\text{argmin}_{\vec{d}_1, \vec{d}_2, \dots}(m_1\|\vec{d}_1\|^2 + m_2\|\vec{d}_2\|^2 + m_3\|\vec{d}_3\|^2 + \dots) & \\
{\small \text{such that }} \; \mathcal{C}(p_1 + \vec{d}_1, p_2 + \vec{d}_2, p_3 + \vec{d}_3, \dots) &
\end{align*}
$$

#### Multiple constraints

Up until now we've talked about enforcing single positional constraints, but how do we deal with **multiple constraints** that may be conflicting?
Since there are many constraints to impose, solving one may break another, so we would need to find a **solution that satisfies all constraints**: this *could* be found analytically by solving the system of all constraints, but it is an extremely computationally expensive process that we don't have time to undertake at each physics step.
Instead, a much more cheap solution is to **enforce the constraints in cascade** one by one during the constraint enforcement phase of the physics step: first we satisfy one, then try to satisfy another and then another and so on, and if one we already satisfied breaks we simply try to satisfy it again starting from the new positions that satisfy all previous constraints.

{% include responsive-image.html path="assets/images/06a_MultipleConstraints.png" alt="Visual representation of the continuous cycle of constraint satisfaction" max-width="400px" %}

We repeat this process until:

- we **converge** to a solution that satisfies all constraints with a max error below a certain threshold...
- ... or we **run out of time** after $$N$$ iterations.

If we time-out the system might be left in a state where some constraints are broken, but this isn't much of an issue: usually these inconsistencies are solved in the next physics step and even if they weren't the system is still soft real-time, so it doesn't matter much.
It can be demonstrated that usually convergence is reached with a relatively small number of iterations (1 to 10) and that to optimize the process we can **solve the most unsatisfied constraints first**, meaning the ones that produce the most error.
The only problem of this method is that it is *sequential*, so it doesn't allow for optimization through the use of parallelism: some parallel approaches have been developed, but although they speed up single iterations they also tend to have worse convergence in practice and thus are not so used.

### Rigidbodies as emerging behaviour

One incredible perk of Position Based Dynamics is that we can use this system not only to simulate particles but also **rigidbodies**, which we get as emerging behaviour through the use of **particles and equidistance constraints**.
This seems too good to be true, but it is: we can get rigidbody behaviour without explicitly keeping track of the orientation of the object, its angular velocity etcetera by simply *placing particles in the vertices of its shape and connecting them with equidistance constraints* that act as solid beams between them, thus creating a rigid object.

Let's see an example of this in action by considering a 2D box made of 4 particles and 6 equidistance constraints falling onto a ledge.
Once one of the particles is set to be inside the ledge, a constraint tries to push it upwards to fix the situation: this, however, breaks the equidistance constraints, which are then solved and result in the object tilting.
The previous positions of each particle are left unchanged though and this gives each particle a new different velocity based on its distance from the point of collision: the object now has an **angular momentum** and *rotational inertia* as emerging behaviour!
Though the use of constraints that disallow compenetration we've also implemented a first basic **collision detection** system that not only detects collisions, but also tries to respond to them in a believable way by making objects bounce on impact (although for more control *impact impulses* would need to be added).

{% include responsive-image.html path="assets/images/06a_Rigidbodies.png" alt="Visual representation of rigidbody behaviour emerging from PBD" %}

Not only that, but if the particles that made the object had different masses our rule for solving constraints by minimizing the change in momentum would make it so that the object would appear to be **rotating around its barycenter**: the most massive particles would be displaced less, thus skewing the pivot around which the object rotates towards them.

However, this isn't a perfect system: first of all, Position Based Dynamics introduces **approximations** by *placing the mass of a rigidbody only on the particles that delimit its shape* rather than throughout the object.
Moreover, PBD can have **scalability issues** since the number of constraints to enforce and particles to track can increase very rapidly as the number of object increases, undermining the system's performance.
What's more interesting though is that some of the info that the system keeps *implicit* is needed by the rest of the game engine and therefore needs to be **extracted**: mainly, what is needed is the *transform* of the rigidbody to update the position and orientation of the related meshes; however, also velocity, angular velocity etc. may be needed for other reasons.
Let's see how some of this information about the rigidbody can be extracted given the *particle compound* that represents it:

$$
\begin{align*}
\text{Mass:} & \qquad m = \sum_{i=1}^{n}{m_i} \\
\text{Position:} & \qquad p = \frac{1}{m} \sum_{i=1}^{n}{m_i p_i} \\
\text{Velocity:} & \qquad \vec{v} = \frac{1}{m} \sum_{i=1}^{n}{m_i \vec{v}_i}
\end{align*}
$$

How to extract the rotation, angular velocity and moment of inertia matrix is beyond the scope of this course.
Note that the *position* in this case refers to the **position of the barycenter**, a very important parameter that allows us to tie in this particle simulation of a rigidbody with the Scene Graph: we can use this position as the **origin of the local space representing the object** to which we can attach the detailed 3D mesh, collision proxy and all other things that should be children of the object in the Scene Graph.
This node then keeps as its "children" the particles that compose it, whose position is defined in local space and updated only by the physics engine, which works in World space and uses this info to extract the **new local transform** of the node, i.e. *its physically-determined position and orientation*.

{% include responsive-image.html path="assets/images/06a_PBDSceneGraph.png" alt="Scene graph with PBD particles in it" max-width="400px" %}

Of course this isn't the only way we can simulate rigidbodies: we could instead explicitly model rotation and angular velocity and use integration systems that account for those parameters too.
In fact, many physics engines use a mixed system, one that can convert from PBD to proper rigidbodies dynamically.

#### Other objects as emerging behaviour

Rigidbodies aren't the only objects whose behaviour naturally emerges from Position Based Dynamics: instead, using the right kind of constraints a number of complicated systems can be simulated.
Just citing a few examples, *articulated bodies* and *ragdolls* can emerge through the use of equidistance constraints and angle constraints for the joints; we then have *non-elastic ropes* simulated through equidistance constraints and fixed positions; *cloth* can also be masterfully simulated with the right constraints.
These simple examples underline the incredible power of Position Based Dynamics.
