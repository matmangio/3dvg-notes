---
title: Textures
---

In the last chapter we described **per-vertex attributes** as a way to "paint" information across a mesh's surface: however, this isn't a great solution for a bunch of different data types.
See, the problem is that per-vertex attributes only offer *granularity up to the vertex level* and are therefore automatically *interpolated across faces*: if we take for example the color of a surface, this only allowed us to render **gradients** of colors on faces and completely shut down more complex designs.

To instead display **rich signals** over a mesh the idea is to store that signal into a **2D image** that is then **wrapped around the surface** described by the mesh.
We call such an image a **texture** (*or texture map*) and to distinguish it from regular 2D images the elements that compose it aren't called pixels but rather **texels**, which are arranged in a *grid*: a texture sheet is basically a *rasterized image*.
They're one of the *most common and important assets* of a game, along with one of the biggest VRAM consumers: most of the visual richness perceived in the typical video game is due to textures!

{% include responsive-image.html path="assets/images/10_TextureExample.png" alt="Example of a texture sheet getting wrapped around a surface" max-height="120px" %}

Texture sheets offer many advantages over per-vertex attributes: instead of a sparse irregular sampling of the original surface's characteristic we have a *dense regular sampling* organized on a 2D grid for each signal (*attribute*) we'd want to store.
The sampling is done at *texel level* and when rendering between samples we'll use **bilinear interpolation** to mix the values recorded in the nearest samples: this guarantees far better results than per-vertex interpolation since it requires a lot less data to be recorded.
If, for example, storing a $$1000 \times 1000$$ texture in memory is relatively cheap, a model with the same level of detail in per-vertex attributes would require over a million vertices!

## Textures sheets

A lot of different signals can be stored in textures and usually we have *one texture sheet per signal*, where the bits and channels representing each texel are used to convey wildly different information depending on the type of texture.
Some **common texture types** in games are the following:

- **Color map** (*or diffuse map/RGB map*): each texel contains a *base color* expressed in the RGB format;
- **Alpha map** (*or cutout texture*): each texel contains a *transparency factor* $$\alpha$$;
- **Normal map**: each texel contains a *normal versor* expressed in its $$(x, y, z)$$ components;
- **Specular map**: each texel contains a *specular coefficient* value used for lighting computations;
- **Glossiness map**: each texel contains a *glossiness value* used for lighting computations;
- **Light map**: each texel contains a *baked lighting value* used to precompute lighting for static objects;
- **Height map** (*or displacement map*): each texel contains a *distance from the surface* value.

Several of these texture sheets can be associated to a mesh and they all need to be present in GPU memory when the model is rendered.
A common structure to ease this process is the use of **material assets**: *each mesh is associated with a material*, which in turn *contains all needed texture sheets* along with *shader files* and *parameters* that will be used during rendering.
Notice that this association isn't necessarily 1:1, but rather *many meshes can use the same material* if they share this part of their appearance: if for example we had a material for glass we could re-use it on every glass object.

{% include responsive-image.html path="assets/images/10_TextureMaterials.png" alt="Schema of the association between textures and materials" max-height="120px" %}

Not only that, but the association isn't even "one texture of a kind per mesh": if different parts of a mesh need different textures we can just split the object in *sub-meshes* each associated with its own material.

Now, texture maps are **assets** and as such they can have asset-related different characteristics.
One of these is the *size* of the texture: this is defined by both its **resolution** as an image, which due to hardware imposed constraints must be a *power of 2* on each side with an hard-wired upper bound (*today is between 2K and 8K*), and the number of **channels** it has, i.e. the *number of values each texel can store*, which is usually between 1 and 4.
Moreover, textures can use different degrees of **compression** to reduce their size both on disk and in GPU VRAM: there are many techniques to do so, like *color quantization* or *compression schemas* designed specifically for textures, such as Microsoft's DXT1-5.
Because of just how much detail they can add to the game world, texture resolution has often a bigger impact on visuals than mesh resolution!

{% include responsive-image.html path="assets/images/10_TextureResolution.png" alt="Example of the impact of different texture resolutions" caption="The same scene with different texture resolutions" max-height="170px" %}

### Mip-maps

Among the characteristics of textures as assets, an odd one stands out: the number of **mip-map levels** the texture has, if any.
**Mip-maps** are the *built-in LOD system for textures*: starting from the original image (LOD-0), a number of lower-resolution versions can be precomputed by downsampling it, thus creating a LOD pyramid for the texture that *reduces its size by $$1/2$$ at each level* down to a 1x1 image.
Often stored together in a single file, mip-maps are then used during rendering by allowing the GPU to select which level texture to use for each pixel on the screen, sometimes using different levels for different parts of the image based on their distance from the viewer.

{% include responsive-image.html path="assets/images/10_MipMaps.png" alt="Example of a mip-map" max-height="150px" %}

What this technique does is basically a *pre-filtering of textures* aimed at avoiding *subsampling artefacts*: if a distant object was rendered using the full texture then its few pixels would likely be associated at very distant texels, completely breaking the texture's structure.
By providing already downsampled versions of the texture in the form of mip-maps we can avoid this problem altogether.

## UV mapping

Now, how do we define *how a given texture is mapped over a given mesh*?
To map each part of the mesh to certain texels what we need is a correspondence between the mesh surface and the 2D **Texture Space**, an imaginary space containing the texture with coordinates in the range $$[0, 1]$$ on each axis: by convention in this space the point $$(0,0)$$ is the top-left corner of the texture (*although OpenGL uses the bottom-left corner as its origin*).

{% include responsive-image.html path="assets/images/10_TextureSpace.png" alt="Visualization of a texture space" max-height="180px" %}

To create this mapping we use what's called a **UV map** (*or "parametrization"*), a *per-vertex attribute that represents each vertex's corresponding position in Texture Space*: this position is different from the vertex's position in the 3D mesh, so to mark that distinction we call **Texture Space coordinates $$u$$ and $$v$$** (hence the term "UV map").
During rasterization these coordinates will be interpolated across faces as any other per-vertex attribute, giving each fragment of the mesh a coordinate in Texture Space that the GPU can then use to read from the texture.
Notice how using *normalized texture coordinates* in the range $$[0,1]$$ allows us to be completely *agnostic of texture resolution or aspect ratio* when building the UV map: this way we can, for example, change the mip-map level of the texture without the need to change the UVs.

### UV map construction

How is the UV map for a mesh constructed?
Although there aren't universally accepted techniques to assign UV coordinates to the vertices of a mesh, an endeavour that falls more on the artistic asset creation side of things, some methods are more diffused than others.
One of this is the **atlas UV map**: the basic idea is to divide the mesh in different **patches of polygons** that are then flattened and mapped to separate **islands** in UV space, which are in turn packed into the texture rectangle.

{% include responsive-image.html path="assets/images/10_TextureAtlas.png" alt="A mesh and the corresponding texture atlas" max-height="200px" %}

Looking at this technique we can immediately spot a problem: how do we deal with the *edges that divide different patches*?
Intuitively they need to be mapped to both points of the atlas that represent them: we therefore need to introduce a **texture seam** (or **texture cut**) which exactly like vertex seams means *duplicating the vertices* defining the edge and giving each a different set of UV coordinates.
In fact, texture seams are almost always necessary to encode the UV map as without them there's no way of completely flattening most 3D meshes in 2D.

{% include responsive-image.html path="assets/images/10_TextureSeams.png" alt="Example of the role of a texture seam in flattening a mesh" max-height="170px" %}

Given that, UV map generation is still a pretty complex task that is typically left to the modeller of the mesh: some aspects of it can be automatized but they still need the attention of an artist to make sure things are properly optimized.
In particular, UV map generation can be split in three phases:

- **Cutting phase**: the most difficult task to to automatize, it requires the artist to **select the cutting edges** that will become texture seams either by directly selecting them or assigning faces to *"texture charts"*.
    While doing so the modeller should **try to reduce the number of cuts** as much as possible as these texture seams can sometimes create artefacts during rendering: this is why cuts they're usually hidden in parts of the mesh that are not as visible.

- **Unfolding phase**: once patches of polygons have been selected they need to be **flattened** onto the 2D texture space, a task that is usually done by automatic algorithms.
    During this process we want to **stretch things as little as possible**, maintaining the general shape of the triangles: this is why the automatic methods employ optimizers that minimize "distortion".

- **Packing phase**: also done by automatic algorithms, this last phase requires us to **pack the islands inside the texture** while minimizing the empty space to reduce the overall storage cost of the texture file.
    During this process we can also perform a sort of **adaptive distribution** of texels: *the more important an area of the texture is the bigger texture space we allocate to it* (more details).

Once all of this is done the artist can evaluate the quality of the resulting UV map using a common trick: the idea is to *superimpose a checkerboard* over the texture and see how it shows up on the model; if the checkerboard is displayed as a regular pattern without distortions then the UV map is good quality.

### Tileable textures

Although we didn't mention it before, UV maps can actually be categorized into two groups: **injective** UV maps, sometimes also called *unwrapped UVs*, and **not-injective** UV maps.
They basically differ in the way they map vertices on the mesh to points in texture space: while **injective UV maps don't allow vertices to share UV coordinates**, creating a 1:1 correspondence between points on the texture and points on the mesh, **not-injective UV maps instead do**, allowing different zones of the mesh to be mapped to the same texture region.
Both approaches are valid and have their unique advantages:

- **Injective UV maps**, being composed only of non-overlapping charts, are very general and **flexible** and are used for several scopes that require injection, like for example light baking or procedurally adding elements to the texture as a result of gameplay (*es. bullet holes on a wall*);

- **Not-injective UV maps** instead let charts of an atlas overlap, which allows them to *exploit symmetries and repetitions* to **save memory space**: this is no small accomplishment since, as we've said, textures are one of the most prominent occupiers of GPU RAM.

Both an injective and not-injective UV map may be needed for a certain mesh: if that's the case then the mesh will have *two UV maps*, with each vertex having *two distinct sets of UV coordinates* $$(UV_A, UV_B)$$.

Having introduced this distinction we can now look at **tileable textures**, a specific type of *not-injective* texture whose purpose is to be *repeated across the mesh*.
This effect is often achieved by using **negative UV coordinates** for the mesh or, more in general, *UV coordinates that exceed the $$[0,1]$$ range*: as we will see later the GPU will "renormalize" these coordinates during rendering, allowing us to visualize polygons that span over multiple repetitions of the texture.

{% include responsive-image.html path="assets/images/10_TileableTexture.png" alt="Example of a triangle spanning multiple texture repetitions" max-height="200px" %}

These types of texture must be constructed with repetition in mind, making sure that the opposite sides wrap around nicely.
When this is done, however, we've obtained a **extremely space-efficient** texture since we will only store a single repetition of it in the VRAM which will then be used for the whole mesh.

{% include responsive-image.html path="assets/images/10_TilingExample.png" alt="Example of a tileable mesh being applied to a large mesh" max-height="120px" %}

## Texture compression

Let's now peak into the life cycle of a texture.
Like any other 3D object in games, the texture starts its life as an **image file on disk**, is *imported onto RAM* as an **image object** and then finally *transferred to the GPU* where it becomes a proper **texture sheet**.

{% include responsive-image.html path="assets/images/10_TextureLife.png" alt="Schema of a texture's life cycle" max-height="150px" %}

The texture sheet data structure is basically a *rasterized buffer of texels* with some peculiarities:

- its *resolution is a power of 2* on each side;
- there may be different *mip-map levels*;
- each texels contains different *channels*, usually 4;
- each texel channel consists of a certain number of *bits*, usually 8;
- it is **compressed following a specific schema**.

This last point is extremely important if we want to save VRAM memory space.
To understand how to compress a texture, however, we first need know how the values from a texture are *read* at runtime by the GPU.
The process of accessing the texture image to retrieve the channel values at a given location ($$(u,v)\rightarrow \mathbb{R}^4$$) is hardwired in the GPU and it takes the name of **per-fragment texture fetch**; divided in numerous steps, this fetch hopefully happens in parallel for all fragments of a mesh at the same time:

1. *Management of out-of-bounds coordinates*, which are returned to the range $$[0,1]$$;
2. *De-normalization of coordinates*, which are turned to the range $$[0, {res}_x] \times [0, {res}_y]$$;
3. *Selection of the appropriate mip-map level* from which to read;
4. ***On-the-fly decompression** of the compressed image data*;
5. *Bilinear interpolation of data* between 4 texels, plus linear across mip-map levels.

The random order at which the texels will need to be accessed means that we need a compression schema that saves VRAM space but also preserves the **random accessibility of texels**.
Some techniques for compression include **color quantization**, i.e. reducing the number of bits per channel to reduce the number of total bits per texel (*eg. 5 for red, 5 for green, 5 for blue, 1 for alpha $$\rightarrow$$ 16 bits per texel*), or **color-table** techniques where a number of colors are stored in a pallette and the texels only contain an index of their color (*eg. 256 color table, 8 bits index per texel*).
We then have, as we said, **specialized compression schemas**: these often have *fixed compression ratios* ($$1/4$$) and are quite *lossy*, giving unfortunately *unfavourable compression/loss ratios* all for the sake of random accessibility.
The most diffused scheme is *S3TC* and its variants *DTX-1* to *DTX-5* (different techniques for alpha values).

It is then interesting to look at which **image format** is most suited for storing textures on disk.
While *normal image formats* can afford to use *excellent compression* methods and *any resolution* since they require to *decompress the entire image* before accessing any pixels and *don't contain mip-map levels* by default (they would need to be procedurally generated), **image formats for textures** have different priorities: they first of all need to be **light to load** and hopefully directly streamable from Disk to RAM and then to GPU VRAM, and secondly need to **contain mip-map levels** that were designed and baked by an artist, even if this means sacrificing some liberty on the resolution and having *worse compression rates*.
There exists such a format, the **.DDS** (*Direct Draw Surface*), which is basically a replica of the format used in the GPU: this means that compression is *very lossy* and with a *bad compression rate*, but on the other hand the image is **GPU ready** with no need to decompression/recompression and it *includes mip-maps* if needed.
Other image formats such as *.JPG/.JPEG*, *.PNG*, *.TIFF* and *.RAW* (rare) and *.PNM* (even rarer) can be used to store textures but although they offer greater compression they have worse results.

## Interesting texture types

Let's now explore more in depth how some texture types are defined and used: some texture sheets hide a great complexity behind them, so it's time to peel the curtain and see what's up with them.

### Alpha maps

**Alpha maps**, also called **cutout textures**, are very simple textures that often contain a *single bit of information per texel* representing whether the corresponding point of the mesh is **transparent or not**: $$1$$ if it is transparent, $$0$$ if it isn't.
When building a mesh it is often easier to create somewhat regular geometries instead of precisely modelling and cutting out the shape: this is where alpha maps come to the rescue, allowing modellers to *precisely carve parts of the mesh without increasing the polygon count*.

{% include responsive-image.html path="assets/images/10_AlphaMap.png" caption="In this example the character's robe is full in the mesh, but it's then refined by the alpha map" alt="Example of an alpha map and the corresponding mesh" max-height="220px" %}

Often used to model complex shapes like grass, fur, foliage, banners etc., this map is sometimes merged with the color map to create a texture that has an RGBa value for each texel.

### Bump maps

Another very interesting type of texture are **bump maps**: this umbrella term indicates all texture sheets meant to model **high-frequency geometric details**, i.e. subtle particulars of the model's shape that couldn't be replicated by the mesh's geometry: think, for example, to the bumpy surface of an orange or the small imperfections across a brick's surface.
The idea is to *keep the mesh low-poly* as this optimizes rendering and instead *create detail using textures*, an approach that has gained the name of "**Texture-for-Geometry**": the rationale is that *texels are far cheaper to store and render than vertices* on a mesh!
The added details may extrude out or go into the mesh's real surface and are usually "fake" in the sense *they **only affect lighting** and don't really modify the geometry of the mesh*: this is usually enough to trick the player's eye, especially when dynamic lights are used.

Based on the way the geometric details are represented we can have two different types of bump maps:

- **Displacement maps**: here the detail is encoded by storing **geometric differences** between the mesh geometry and the detailed surface as either *scalars or vectors*, which can then be used for on-the-fly re-tessellations or *parallax mapping*.
- **Normal maps**: here details are encoded by storing the **surface normals** of the detailed surface, which are only used to fake geometric complexity through lighting computations.

Of course these two are not exclusive and a mesh can have both a displacement map and a normal map.
Let's now see what they are more in detail.

#### Displacement maps

As we've already said, a **displacement map** stores in a texture the **distance** between the detailed surface an the plain geometry of the mesh as either a scalar or vector.
Let's consider the **scalar** case first and observe an example with a screw head sticking out from a table's surface: the real surface has a bump where the screw is but the table's mesh does not, so in the scalar displacement map we *record for each texel the distance along the **face's normal** of the mesh from the real surface*.

{% include responsive-image.html path="assets/images/10_ScalarDisplacementMap.png" alt="Visualization of the difference between a surface and a mesh and the resulting displacement map" max-height="250px"%}

As we can see from the figure above the resulting texture is a **greyscale image** since for each texel we need only record a single floating point value so we have **only 1 channel per texel**: this single channel can either contain an unsigned value, in which case we have to decide in advance if the numbers represent *extrusions or excavations*, or a signed value, in which case we can do both.
Easy to paint and manipulate by artists, this texture can be used for a number of different things, from *ambient occlusion* (*see global illumination*) to using the implied normals for direct lighting (not common anymore), and it can also be used as intermediate data for the construction of normal maps.

A technique specific for displacement maps is what's called **parallax mapping** where in a step during rendering the *geometry of the mesh is actually modified according to the displacement map*, generating real detail on the model.
Although this is quite expensive it has the added bonus of *affecting the silhouette of the object*, something that normal maps and other effects that try to fake details can't do.

{% include responsive-image.html path="assets/images/10_ParallaxMapping.png" alt="Example of the parallax mapping technique in use" max-height="150px"%}

Finally, displacement maps can also record the difference between the detailed surface and the mesh as **vectors**: this is much more expressive as it allows for *overhangs* and more complex structures but it is also **wildly expensive** and therefore not very used in games (yet).

{% include responsive-image.html path="assets/images/10_VectorDisplacementMap.png" alt="Visualization of a vectorial displacement map" max-height="180px"%}

#### Normal maps

A **normal map** is a texture in which each texel stores the **normal vector** the detailed surface has in the corresponding point as a *collection of cartesian coordinates*, one for each channel (*although there exists better ways to record unit vectors*): these vectors are then used during **lighting computations** to create the illusion of additional detail on the surface of the model that isn't actually there.
Differently from displacement maps, in fact, normal maps *don't affect the silhouette of the object* or it's parallax, meaning that if the viewer gets too close or watches the surface from a acute enough angle they will see the trick: despite this the effect is usually **pretty convincing** as long as the detail we're trying to fake isn't macroscopic and we have an adequate mesh/texture resolution.
The lighting is in fact able to trick the eye notwithstanding the absence of real geometric detail.

{% include responsive-image.html path="assets/images/10_NormalMap.png" alt="Visualization of the difference between a surface and a mesh and the resulting normal map" max-height="250px"%}

It's not the first time that we use normals different from the real ones of the 3D mesh for lighting: when talking about meshes we introduced the concept of *per-vertex normals* as a way of creating the illusion of a smooth surface even though the low-poly model is actually blocky.
What's the difference from normal maps?
Simple: while per-vertex normals were automatically interpolated across faces by the rasterizer and therefore were *only able of creating curvature*, **normal maps can create whatever details on the surface** as they're directly mapped to the fragments that create the final render.

{% include responsive-image.html path="assets/images/10_NormalMapVSAttribute.png" alt="Visualization of the difference between per-vertex normals and mapped normals" max-height="220px"%}

##### <big><u><b>Tangent space</b></u></big>

An interesting question is the following: in *which space* are the normals that constitute a normal map encoded?
A first idea could be to record these vectors in *Local space*, creating what we call an **Object-Space Normal Map** (**OSNM**): this way texture normals are expressed in the same space as mesh vertices.
This choice has the following effects:

- **per-vertex normals are unnecessary**, being completely substituted by texture normals;
- **normals are trivial to apply** during rendering as we can simply read the texture to obtain the value;

    $$\hat{n}' = \begin{pmatrix}n_x \\ n_y \\ n_y\end{pmatrix}$$

- **normal maps are bound to specific objects** since Object Space is relative to the single mesh: this greatly *reduces re-usability*, requiring us to create a new normal map for every object;
- **normal maps are injective** as each region of the map needs to be bound to one specific area of the object: even if the object presents symmetries these cannot be exploited as the normals in the two parts would be opposite.
    This also means *no tileable normal maps* for big surfaces.

{% include responsive-image.html path="assets/images/10_OSNM.png" alt="Object-Space Normal Map of a horse mesh" caption="Object-Space Normal Map of a horse mesh" max-height="150px"%}

Suddenly Object-Space Normal Maps don't seem like a great idea anymore and as a matter of fact they're rarely used in games.
Instead, texture normals are defined in **Tangent Space** (*or TBN Space*), a new vector space that is unique to each point on the mesh as it is *defined by the **per-vertex normal** ($$z$$) and the **tangent** ($$x$$) and **bi-tangent** ($$y$$) vectors*, the last two being vectors that are parallel to the surface and orthogonal to one another and to the per-vertex normal.
As we will see, the tangent and bi-tangent vectors are first computed across faces and then averaged at vertices, similarly to per-vertex normals.

{% include responsive-image.html path="assets/images/10_TangentSpace.png" alt="Visualization of the tangent space in a point on the mesh" max-height="120px"%}

By defining the texture normal in this space what we're doing is basically telling the GPU **how to modify the per-vertex normal** instead of replacing it.
Now, to have this tangent space available in each point of the mesh we have to make its representation a bit more heavy storage-wise by also storing **tangent and bi-tangent versors as per-vertex attributes** so that they will be interpolated across the surface during rasterization, allowing us to use them when reading from the texture to interpret the normals.
In reality optimizations are possible: instead of recording normal, tangent and bi-tangent all together, we can *just store normal and tangent and get the bi-tangent as the cross product of those two* in each point, also ensuring that they form an (almost) orthonormal basis (just need to store the direction as a single bit).
However, this **increase in memory occupancy** is largely repayed by a number of wonderful properties of **Tangent-Space Normal Maps** (**TSNM**):

- **normal maps can be not-injective**, allowing us to take advantage of *symmetries and repetitions*: since the normals are expressed in Tangent space they will be automatically adjusted to the orientation of the vertex normal;
- **normal maps can be shared between different objects** because they have no reference to the object itself but rather to its normals: this way, for example, a concrete cube and a concrete ball can use the same normal map to represent the bumps on the surface;
- **normal maps are independent from the mesh** and can be constructed without knowing the object to which they'll be applied, allowing us to optimize the asset production pipeline.

{% include responsive-image.html path="assets/images/10_TSNM.png" alt="Tangent-Space Normal Map of a horse mesh" caption="Tangent-Space Normal Map of the same horse mesh as before" max-height="150px"%}

These incredibile characteristics make Tangent-Space Normal maps the norm in 3D video games, the only small complication being that when **applying** them we first have to *read from the texture the $$(n_x, n_y, n_z)$$ coordinates, multiply them by the corresponding interpolated axis and sum them* to obtain the normal in Object space needed for lighting computations.

$$\hat{n}' = \begin{bmatrix}\vec{T} \; | \; \vec{B} \; | \; \vec{N}\end{bmatrix} \begin{pmatrix}n_x \\ n_y \\ n_z\end{pmatrix} = n_x\vec{T} + n_y \vec{B} + n_z \vec{N}$$

##### <big><u><b>Normal map storage</b></u></big>

How are the values for normal maps stored in textures?
Usually the idea is to use an **RGB texture** where each channel is linked to a component of the normal: red is $$x$$, green is $$y$$ and blue is $$z$$.
The only problem here is that the coordinates of normals are in the range $$[-1,+1]$$ while color channels store values in the interval $$[0,1]$$: we therefore need a linear mapping which is simply done by adding 1 and dividing by 2.

$$R = \frac{x + 1}{2}$$

Notice how when using Tangent-Space Normal Maps the normal axis is linked to the blue color, which is why *normal maps usually appear blueish*: the normal component is usually the strongest and therefore blue is the most prevalent color.
Object-Space Normal Maps don't have this characteristic and are therefore much more colorful.

##### <big><u><b>Tangent an Bi-tangent extraction</b></u></big>

How are the *tangent and bi-tangent versors computed* for each vertex?
The basic idea is that these directions are actually first **computed for faces** and then **averaged at each vertex** similarly to what happened for per-vertex normals: this step takes during *mesh preprocessing* since tangent and bi-tangent must be set as per-vertex attributes.

Now, to visualize the tangent and bitangent directions on faces we can imagine of creating a grid texture and applying it on the mesh: for each face the *tangent direction is the horizontal line* and the *bi-tangent direction is the vertical line*; **tangent and bi-tangent directions are extracted from the UV-map**.
Given a UV-map the idea is then this: for each face project their points in texture space and find the linear combinations of projected edges that give respectively the **horizontal versor** and **vertical versor**; using the same parameters for a linear combination of edges in 3D will yield the **tangent** and **bi-tangent** versors respectively.

{% include responsive-image.html path="assets/images/10_TangentsExtraction.png" alt="Visualization of the correspondence between edges and tangents" max-height="200px" %}

So given the points $$p_0, p_1, p_2$$ that make up the triangle and their projections $$q_0, q_1, q_2$$ we first need to find the vectors $$\vec{e}_1, \vec{e}_2$$ representing the edges in 3D and their projections $$\vec{t}_1, \vec{t}_2$$ in Texture Space.
Once this is done we need to find the scalar coefficients $$a, b, c, d$$ such that:

$$a\vec{t}_1 + b\vec{t}_2 = \vec{u} = \begin{pmatrix}1 \\ 0\end{pmatrix} \qquad c\vec{t}_1 + d\vec{t}_2 = \vec{v} = \begin{pmatrix}0 \\ 1\end{pmatrix}$$

This can be simply done using 2x2 matrix inversion since we have the identity matrix:

$$\begin{bmatrix}\vec{t}_1 \; | \; \vec{t}_2\end{bmatrix} \begin{bmatrix}a & c \\ b & d\end{bmatrix} = \begin{bmatrix}1 & 0 \\ 0 & 1\end{bmatrix} \; \longrightarrow \; \begin{bmatrix}a & c \\ b & d\end{bmatrix} = \begin{bmatrix}\vec{t}_1 \; | \; \vec{t}_2\end{bmatrix}^{-1}$$

We can then use these coefficients to determine the **tangents and bi-tangents** for the triangle:

$$\vec{T} = a\vec{e}_1 + b\vec{e}_2 \qquad \vec{B} = c\vec{e}_1 + d\vec{e}_2$$

#### $$\mu$$-meshes

Bump maps have also inspired an entirely new way of defining meshes called **$$\bf \mu$$-meshes** or **micro-meshes**: pioneered by Nvidia, this technique for defining meshes is based on the idea of having *very coarse base mesh that is then **refined and displaced** on-the-fly* at some point of the rendering pipeline using a *specially formatted scalar displacement map*.

{% include responsive-image.html path="assets/images/10_MicroMeshes.png" alt="Example of micro-mesh refinement and displacement" max-height="100px" %}

Let's see the two steps of this dynamic displacement in more detail:

- **Refinement**: each triangle of the mesh is subdivided in smaller triangles up to a maximum of 64 points of subdivision per edge ($$64^2$$ triangles), with the possibility of using different numeric subdivisions on different edges to better align with other triangles that may have been subdivided less (*mixed resolution*).

- **Displacement**: using a special, hardware-supported format each triangle's new vertices are displaced using a *scalar displacement map* created specifically for that base-mesh face that uses *per-vertex custom displacement vectors* instead of the face normal.
    This technique doesn't require UV maps and therefore doesn't even need tangent computations; moreover, since we're using a displacement there's an actual change in geometry that reflects both on silhouette and lighting.

These micro-meshes are **extremely efficient**, costing as low as 1 byte per triangle in VRAM space and being natively optimized for raytracing.
Moreover they have a built-in LOD mechanism based on how much we subdivide each face all the way down to the base mesh.

## Creating textures

Let's talk a bit about **creating texture maps** of different kinds.
Texture creation is usually incorporated as a part of the *asset production pipeline* for a specific model and it can be the result of the work of an artist or an automatic algorithm depending on the type of map being produced.
Many different techniques and approaches can be employed when creating texture sheets, some of which we'll see in this chapter.

Let's start by looking at how to create the most common type of map, **color maps**.
The process can be very different depending on the specific situation:

- *Image first, then UV map*: in case where the texture already exists, for example when applying a *logo/photo* to a mesh or when using a *tileable texture*, it's the task of a *UV mapper* to create a UV map to correctly display the texture.

- *UV map first, then paint in 2D*: given the UV map of a mesh, a *2D artist* manually paints a texture over it in a 2D image editing software (*eg. photoshop*).
    This requires a bit of abstraction to imagine how the texture will appear once applied to the mesh.

- *UV map first, then paint in 3D*: the most common and easy choice, here the mesh is directly painted over inside the 3D modelling software by a *3D painter* like as if it was a physical sculpture being painted.
    The artist's brush strokes are then unwrapped using the UV map to create the texture.

- *Paint over hi-res mesh, then bake texture*: when a high-resolution version of the mesh is available another possible way of creating a texture is to first *paint the vertex attributes* in 3D, then *coarsen* the mesh to obtain the low-poly version of which we create an *injective UV map*.
    Using this map we can finally **bake** the texture using the values painted in the high-poly version of the mesh (*see later*).

As we can see there are many different ways of creating textures as simple as the color map.
In the next paragraph we instead explore more in detail a number of techniques for building *normal maps*.

### Authoring normal maps

Due to their intrinsic nature relating to the surface of the mesh, **normal maps** are a tad more complicated to author as they must be the result of some process taking in consideration the *geometry of the desired surface*.
While it is true that exactly like any other texture the normal maps can be **painted on the 3D mesh** using appropriate brushes in a modelling software through a technique similar to sculpting, but where instead of modifying the geometry the system only updates the normals on the texture map, there are also many more interesting ways of creating texture sheets of this kind.
In this paragraph we take into consideration some of these different algorithms.

#### From a displacement map

Once we have a **displacement map** that associates each texel to a height value which is quite easy to paint over the model, a normal map of the same resolution can be extracted from it with relative ease by using the following procedure: for each texel $$t$$ of the displacement map, compute the **best fitting plane** around it by taking the height values of the surrounding texels in a 3x3 grid (*or 5x5, or 7x7..., $$t$$ included*), translating them in 3D points with coordinates $$(u,v,\text{height}[u,v])$$ and finding the plane *minimizing the sum of squared distances* from them; the **normal of this plane** is the normal in texel $$t$$.

{% include responsive-image.html path="assets/images/10_DisplacementToNormal.png" alt="A displacement map being turned into a normal map" max-height="150px" %}

As we can see this is simply an optimization problem for each texel, which can be solved quite easily.
Moreover, the resulting normals are **already in Tangent Space** by construction, a big pro of Tangent-Space Normal Maps: it can, of course, be turned into an Object-Space Normal Map if needed.

#### From real photos

If we're trying to accurately model a real object one way to create normal maps is to *see how it reacts to light* and basically copy that effect in the texture.
This technique is called **photometric stereo** and is a computer vision technique that compute a sort of "inverse lighting": the algorithm is given in input $$n$$ real images of the object from the **same viewpoint but with different illumination**, which is hopefully controlled and known.
From this it outputs a normal map expressed in "image space" which can then be converted in tangent space or object space depending on the type of normal map wanted.

{% include responsive-image.html path="assets/images/10_PhotoToNormal.png" alt="A set of real photos turned into a normal map" max-height="150px" %}

While traditionally this method required a large number of images done in a specific an known lighting environment, in recent years techniques have been proposed that through the power of Machine Learning may be able to extract normal maps from a single image with natural illumination.
It's an active field of research and one to keep an eye on!

#### From a procedural algorithm

Although it's not very frequent, normal maps can of course be generated **procedurally**.
This has the usual pros and cons of procedurality: a procedural normal map *saves RAM space but costs computing power*, it *can be built at runtime* and can even be *baked in preprocessing* to become an asset.
It is also extremely unlikely to be prone to repetition artifacts and can be easily animated if the surface requires it: the problem is they're *very difficult to control* and as such are not used very often.

{% include responsive-image.html path="assets/images/10_ProceduralNormal.png" alt="An example of a procedural normal map" max-height="100px" %}

#### From an high-resolution model

If we have an high-resolution model of the object available, obtained for example through *sculpting a low-poly mesh* or being it the original model from which a low-poly version was though *automatic simplification*, we can use a technique called **textures baking** to create the normal map along with many other types of textures.
Given an untextured *high-resolution mesh with per-vertex attributes* and a *low-poly version with a UV-map* in input, through a fully automated process we can obtain **textures for the low-poly mesh that store the per-vertex attributes of the high-poly model**.
Notice how the high-resolution mesh doesn't need a UV map since it has so many triangles that it can create detail through per-vertex attributes alone: so for example per-vertex normals can be turned in Object-Space Normal Maps (*which can then be converted to Tangent Space*), base colors in color maps, baked lighting or ambient occlusion to light maps and AO-maps and so on.
Moreover, through the simple distance between the two meshes we can create a *displacement map*, no per-vertex attributes needed.

But how does the baking process work?
It is actually quite simple: using the UV-map we map each one of the *low-poly mesh triangles in Texture Space*, finding out *which texels it covers* through rasterization.
For each of these texels we then find its *corresponding coordinates on the low-poly model* using an inverse of the UV-map function: these coordinates are then approximated to a **point on the high-poly mesh** whose attributes are *sampled and stored in the texel*.

{% include responsive-image.html path="assets/images/10_BakingTextures.png" alt="Visualization of how textures are baked" max-height="200px" %}

It is now obvious why this process only produces Object-Space Normal Maps and **injective textures** in general: each texel must correspond to a single point on the high-poly mesh (*function from local space to texture space needs to have an inverse*).

### Procedural textures

Of course textures can also be **procedural**, meaning that instead of having an image of texels to read from we instead use a *function that given certain UV coordinates returns a value*: computed during the rendering for each pixel by the *fragment shader*, these procedural maps replace texture fetches!

$$f\begin{pmatrix}u \\ v\end{pmatrix} = \begin{pmatrix}r \\ g \\ b\end{pmatrix}$$

Procedural textures reduce the RAM storage cost of a texture to almost nothing at the expense of GPU usage, which now needs to compute a function for each pixel: they on the other hand have the added benefit of being *resolution independent*, but are also *difficult to control* and author and therefore usually used only for simple effects and images.

### Solid textures

Up until now we've discussed of textures as 2D arrays of samples represented by images: this is a useful abstraction since with most meshes we're only interested to model the surface of the object, which is ostensibly a 2D structure.
But what if we wanted to model the **inside of an object** as well?
Well in that case we would use a **solid texture**, a *3D array of texels* or better a *volumetric voxellized texture*: a **voxel** is a 3D texel, so we can have for example *solid color maps that also describe object's color on the inside*.

{% include responsive-image.html path="assets/images/10_SolidTextures.png" alt="Example of an object using solid textures" max-height="150px" %}

These types of textures are useful for representing *fractures* in objects and as all textures they need to be kept in VRAM, can have mip-maps and so on: interestingly, a solid texture **doesn't need a UV-map** as it is simply *indexed by the geometric mesh*, meaning we use the $$(x,y,z)$$ coordinates of the point on the model to read the sample.
The problem with these is that they're a lot **more expensive** than traditional textures and require space that is *cubic with the resolution*: a possibile solution to this is to use **procedural solid textures** that only require a function evaluation and don't occupy memory.

$$f\begin{pmatrix}x \\ y \\ z\end{pmatrix} = \begin{pmatrix}r \\ g \\ b\end{pmatrix}$$
